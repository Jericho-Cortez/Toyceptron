"""
Tests manuels avec calculs v√©rifiables
Toyceptron - JOUR 2 √âtape 2.4
"""
import random
from neuron import Neuron
from layer import Layer
from activations import identity, relu, heaviside, sigmoid


print("=" * 60)
print("TEST MANUEL - √âtape 2.4 : Tests unitaires basiques")
print("=" * 60)


# ========================================
# Test 1 : Neurone avec poids fix√©s
# ========================================
print("\n[Test 1] Neurone avec poids fixes - Calcul v√©rifiable")
print("-" * 60)

n = Neuron(weights=[1, 1], bias=0, activation=identity)
result = n.forward([2, 3])

print(f"Configuration : weights=[1, 1], bias=0, activation=identity")
print(f"Calcul manuel : 1*2 + 1*3 + 0 = 5")
print(f"R√©sultat obtenu : {result}")
assert result == 5, f"‚ùå ERREUR : attendu 5, obtenu {result}"
print("‚úÖ Test 1 r√©ussi : Calcul correct")


# ========================================
# Test 2 : Neurone avec activation ReLU
# ========================================
print("\n[Test 2] Neurone avec ReLU - Valeur n√©gative")
print("-" * 60)

n_relu = Neuron(weights=[1, -2], bias=-1, activation=relu)
result_relu = n_relu.forward([1, 2])

print(f"Configuration : weights=[1, -2], bias=-1, activation=relu")
print(f"Calcul manuel : 1*1 + (-2)*2 + (-1) = 1 - 4 - 1 = -4")
print(f"Apr√®s ReLU : max(0, -4) = 0")
print(f"R√©sultat obtenu : {result_relu}")
assert result_relu == 0, f"‚ùå ERREUR : attendu 0, obtenu {result_relu}"
print("‚úÖ Test 2 r√©ussi : ReLU fonctionne correctement")


# ========================================
# Test 3 : Porte logique AND avec Heaviside
# ========================================
print("\n[Test 3] Porte logique AND - Application concr√®te")
print("-" * 60)

# Un neurone peut impl√©menter une porte logique AND !
n_and = Neuron(weights=[1, 1], bias=-1.5, activation=heaviside)

test_cases = [
    ([0, 0], 0),  # 0 + 0 - 1.5 = -1.5 ‚Üí heaviside ‚Üí 0
    ([1, 0], 0),  # 1 + 0 - 1.5 = -0.5 ‚Üí 0
    ([0, 1], 0),  # 0 + 1 - 1.5 = -0.5 ‚Üí 0
    ([1, 1], 1),  # 1 + 1 - 1.5 = 0.5 ‚Üí 1
]

print("Table de v√©rit√© AND :")
print("A  B  | Sortie attendue | Sortie obtenue | Statut")
print("-" * 60)

all_passed = True
for inputs, expected in test_cases:
    result = n_and.forward(inputs)
    status = "‚úÖ" if result == expected else "‚ùå"
    print(f"{inputs[0]}  {inputs[1]}  |        {expected}        |       {result}        | {status}")
    
    if result != expected:
        all_passed = False
        print(f"   ‚ùå Erreur sur {inputs} : attendu {expected}, obtenu {result}")

if all_passed:
    print("‚úÖ Test 3 r√©ussi : Le neurone impl√©mente correctement AND !")


# ========================================
# Test 4 : Layer avec calcul manuel
# ========================================
print("\n[Test 4] Layer - V√©rification d'une couche compl√®te")
print("-" * 60)

# Cr√©er une couche avec des poids contr√¥l√©s
random.seed(999)  # Seed fixe pour reproductibilit√©
layer_test = Layer(num_neurons=2, num_inputs=2, activation=identity)

print(f"Couche avec 2 neurones, 2 inputs")
print(f"Neurone 1 : weights={layer_test.neurons[0].weights}, bias={layer_test.neurons[0].bias:.3f}")
print(f"Neurone 2 : weights={layer_test.neurons[1].weights}, bias={layer_test.neurons[1].bias:.3f}")

inputs_test = [1.0, 0.5]
result_layer = layer_test.forward(inputs_test)

print(f"\nEntr√©es : {inputs_test}")
print(f"Sorties : {result_layer}")
print(f"Type : {type(result_layer)}, Longueur : {len(result_layer)}")

assert len(result_layer) == 2, "‚ùå La couche doit retourner 2 sorties"
assert isinstance(result_layer, list), "‚ùå forward() doit retourner une liste"
print("‚úÖ Test 4 r√©ussi : La couche fonctionne correctement")


# ========================================
# Test 5 : Reproductibilit√© avec seed
# ========================================
print("\n[Test 5] Reproductibilit√© - M√™me seed = M√™mes poids")
print("-" * 60)

random.seed(42)
layer1 = Layer(num_neurons=3, num_inputs=2, activation=identity)
weights1 = [n.weights for n in layer1.neurons]
result1 = layer1.forward([1.0, 2.0])

random.seed(42)  # Reset avec le m√™me seed
layer2 = Layer(num_neurons=3, num_inputs=2, activation=identity)
weights2 = [n.weights for n in layer2.neurons]
result2 = layer2.forward([1.0, 2.0])

print(f"Layer 1 poids : {weights1}")
print(f"Layer 2 poids : {weights2}")
print(f"R√©sultat 1 : {result1}")
print(f"R√©sultat 2 : {result2}")

assert result1 == result2, "‚ùå Les r√©sultats doivent √™tre identiques avec le m√™me seed"
print("‚úÖ Test 5 r√©ussi : Reproductibilit√© garantie")


# ========================================
# Test 6 : Gestion d'erreur
# ========================================
print("\n[Test 6] Gestion des erreurs - Param√®tres invalides")
print("-" * 60)

try:
    n_error = Neuron(weights=None, num_inputs=None)
    print("‚ùå √âCHEC : L'erreur ValueError n'a pas √©t√© lev√©e")
except ValueError as e:
    print(f"‚úÖ Erreur correctement lev√©e : {e}")


# ========================================
# Test 7 : Sigmoid - Valeur m√©diane
# ========================================
print("\n[Test 7] Activation Sigmoid - Valeur z=0")
print("-" * 60)

n_sigmoid = Neuron(weights=[1, -1], bias=0, activation=sigmoid)
result_sigmoid = n_sigmoid.forward([2, 2])

print(f"Configuration : weights=[1, -1], bias=0")
print(f"Calcul : 1*2 + (-1)*2 + 0 = 0")
print(f"Sigmoid(0) devrait √™tre proche de 0.5")
print(f"R√©sultat obtenu : {result_sigmoid:.4f}")

# Tol√©rance de 0.01 car sigmoid(0) = 0.5
assert abs(result_sigmoid - 0.5) < 0.01, f"‚ùå Sigmoid(0) devrait √™tre ~0.5, obtenu {result_sigmoid}"
print("‚úÖ Test 7 r√©ussi : Sigmoid fonctionne correctement")


# ========================================
# R√âCAPITULATIF FINAL
# ========================================
print("\n" + "=" * 60)
print("üéâ TOUS LES TESTS UNITAIRES SONT PASS√âS !")
print("=" * 60)
print("\n‚úÖ Neuron : Calculs corrects")
print("‚úÖ Activations : identity, relu, heaviside, sigmoid")
print("‚úÖ Layer : Forward pass fonctionnel")
print("‚úÖ Porte logique AND : Impl√©mentation r√©ussie")
print("‚úÖ Reproductibilit√© : Gestion du seed OK")
print("‚úÖ Gestion d'erreurs : Robuste")
print("\nüöÄ Tu es PR√äT pour l'√©tape 3.1 : Network !")
print("=" * 60)
