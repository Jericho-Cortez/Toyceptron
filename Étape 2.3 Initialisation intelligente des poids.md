#  Pourquoi cette étape est cruciale ?

Jusqu'à maintenant, tu initialisais manuellement les poids de tes neurones :

```python
n = Neuron(weights=[0.5, -0.3], bias=0.1)
```

**Problème** : Dans un vrai réseau avec 100 neurones et 50 entrées, tu ne vas pas écrire 5000 poids à la main !

**Solution** : Génération automatique de poids aléatoires.

***

##  Problème initial : Tous les poids à 0.5

Si tu initialises tous les poids avec la même valeur :

```python
#  MAUVAIS
self.weights = [0.5, 0.5, 0.5]
```

**Conséquences** :

- Tous les neurones d'une même couche apprennent la même chose (symétrie)
- Le réseau ne peut pas capturer des patterns complexes
- Sur TensorFlow Playground, les neurones resteraient identiques

***

##  Solution 1 : Poids aléatoires

### Implémentation dans `neuron.py`

Modifie le constructeur de `Neuron` pour accepter `None` comme valeur de `weights` :

```python
import random

class Neuron:
    def __init__(self, weights=None, num_inputs=None, bias=0, activation=identity):
        """
        Paramètres :
        - weights : Liste de poids (si None, ils seront générés)
        - num_inputs : Nombre d'entrées (nécessaire si weights=None)
        - bias : Biais du neurone
        - activation : Fonction d'activation
        """
        
        if weights is None:
            # Génération automatique de poids aléatoires
            if num_inputs is None:
                raise ValueError("Si weights=None, num_inputs doit être fourni")
            
            self.weights = [random.uniform(-1, 1) for _ in range(num_inputs)]
            self.bias = random.uniform(-1, 1)
        else:
            # Poids fournis manuellement
            self.weights = weights
            self.bias = bias
        
        self.activation = activation
```


### Explication du code

**1. Paramètre `weights=None`** :

- Si tu fournis des poids → ils sont utilisés directement
- Si tu ne fournis rien → ils sont générés automatiquement

**2. `random.uniform(-1, 1)`** :

- Génère un nombre aléatoire entre -1 et 1
- Distribution uniforme (toutes les valeurs ont la même probabilité)

**3. Protection d'erreur** :

- Si `weights=None` ET `num_inputs=None` → impossible de savoir combien de poids générer
- Le code lève une erreur explicite

***

##  Test de la génération aléatoire

Crée un fichier de test rapide :

```python
# test_random_weights.py
from neuron import Neuron

# Test 1 : Génération automatique
print("\n[Test 1] Génération automatique de poids")
n1 = Neuron(num_inputs=3)
print(f"Poids générés : {n1.weights}")
print(f"Biais généré : {n1.bias}")

# Test 2 : Chaque neurone a des poids différents
print("\n[Test 2] Diversité des poids")
n2 = Neuron(num_inputs=3)
n3 = Neuron(num_inputs=3)
print(f"Neurone 2 : {n2.weights}")
print(f"Neurone 3 : {n3.weights}")
print(" Les poids sont différents" if n2.weights != n3.weights else "❌ Problème")

# Test 3 : Poids manuels toujours possibles
print("\n[Test 3] Poids manuels")
n4 = Neuron(weights=[1, 1], bias=0)
print(f"Poids manuels : {n4.weights}")
```

**Sortie attendue** :

```
[Test 1] Génération automatique de poids
Poids générés : [0.347, -0.821, 0.562]
Biais généré : -0.234

[Test 2] Diversité des poids
Neurone 2 : [-0.678, 0.912, -0.145]
Neurone 3 : [0.456, -0.789, 0.234]
 Les poids sont différents

[Test 3] Poids manuels
Poids manuels : [1, 1]
```


***

##  Reproductibilité avec `random.seed()`

### Problème : Résultats non reproductibles

Si tu lances ton code 2 fois, les poids changent :

```python
# Exécution 1
n = Neuron(num_inputs=2)
print(n.weights)  # [0.347, -0.821]

# Exécution 2 (nouveau lancement)
n = Neuron(num_inputs=2)
print(n.weights)  # [-0.456, 0.912]  ← Différent !
```

**Conséquence** : Impossible de déboguer ou comparer des résultats.[^1]

### Solution : Fixer le seed aléatoire

```python
import random

random.seed(42)  # Fixe l'aléatoire

n1 = Neuron(num_inputs=3)
print(n1.weights)  # [0.278, -0.949, 0.784]

# Relance avec le même seed
random.seed(42)
n2 = Neuron(num_inputs=3)
print(n2.weights)  # [0.278, -0.949, 0.784] ← Identique !
```

**Utilisation dans `layer.py`** :

```python
# layer.py
import random

class Layer:
    def __init__(self, num_neurons, num_inputs, activation=identity):
        random.seed(42)  # ← OPTIONNEL, pour tests reproductibles
        self.neurons = []
        
        for _ in range(num_neurons):
            neuron = Neuron(num_inputs=num_inputs, activation=activation)
            self.neurons.append(neuron)
```

** Important** : En production, **ne pas mettre de seed** (les poids doivent être différents à chaque entraînement). Le seed est utile **uniquement pour déboguer**.[^1]

***

##  Pourquoi `uniform(-1, 1)` ?

### Comparaison des plages d'initialisation

| Plage | Avantages | Inconvénients |
| :-- | :-- | :-- |
| `[0, 1]` | Simple | Tous les poids positifs → biais dans le réseau |
| `[-1, 1]` | Équilibré (positifs + négatifs) | Distribution uniforme pas toujours optimale |
| `[-0.5, 0.5]` | Plus stable pour petits réseaux | Peut être trop faible pour grands réseaux |

**Pour Toyceptron** : `uniform(-1, 1)` est un bon compromis.[^1]

***

##  Stratégies avancées (BONUS - hors scope du projet)

### Initialisation Xavier (Glorot)

Adaptée pour **sigmoid** et **tanh** :

```python
import math

def xavier_init(num_inputs):
    limit = math.sqrt(6 / num_inputs)
    return random.uniform(-limit, limit)
```


### Initialisation He

Adaptée pour **ReLU** :

```python
def he_init(num_inputs):
    std = math.sqrt(2 / num_inputs)
    return random.gauss(0, std)
```

**Pourquoi ces stratégies existent ?**

- Éviter les gradients qui explosent ou disparaissent (important pour le backpropagation)
- Toyceptron n'a pas de backprop → `uniform(-1, 1)` suffit[^1]

***

##  Checklist de l'étape 2.3

Avant de passer à l'étape 2.4, vérifie que :

- [x] `Neuron` accepte `weights=None` et `num_inputs`
- [x] Les poids sont générés automatiquement avec `random.uniform(-1, 1)`
- [x] Le biais est également généré aléatoirement
- [x] Une erreur claire est levée si `weights=None` et `num_inputs=None`
- [x] Tu comprends le rôle de `random.seed()` (reproductibilité)
- [x] Tu peux créer des neurones avec poids aléatoires **ET** manuels

***

##  Mise à jour de `layer.py`

Maintenant que `Neuron` supporte l'initialisation automatique, `Layer` devient ultra-simple :

```python
# layer.py
from neuron import Neuron

class Layer:
    def __init__(self, num_neurons, num_inputs, activation):
        self.neurons = []
        
        for _ in range(num_neurons):
            # Plus besoin de spécifier les poids !
            neuron = Neuron(num_inputs=num_inputs, activation=activation)
            self.neurons.append(neuron)
    
    def forward(self, inputs):
        outputs = []
        for neuron in self.neurons:
            output = neuron.forward(inputs)
            outputs.append(output)
        return outputs
```

**Test** :

```python
from activations import relu

layer = Layer(num_neurons=3, num_inputs=2, activation=relu)
result = layer.forward([1.0, 2.0])
print(result)  # [0.456, 0.0, 1.234] ← 3 valeurs différentes
```



