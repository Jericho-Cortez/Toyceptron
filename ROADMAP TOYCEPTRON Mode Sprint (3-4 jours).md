## üìå Philosophie de la roadmap

- **Code d'abord, comprend en faisant**
    
- **Validation √† chaque √©tape** (tests rapides)
    
- **Pas de perfectionnisme** : on vise le fonctionnel avant l'√©l√©gant
    
- **R√©vision POO int√©gr√©e** dans chaque √©tape
    

---

## ‚è±Ô∏è JOUR 1 : Setup + Neurone (3-4h)

## ‚úÖ √âtape 1.1 : Remise √† niveau POO Express (30 min)

**Objectif** : Rafra√Æchir les bases Python et POO n√©cessaires au projet[playground.tensorflow+1](https://playground.tensorflow.org/)

**Actions concr√®tes :**

- Ouvre [https://learnxinyminutes.com/python/](https://learnxinyminutes.com/python/) et relis rapidement :
    
    - Les listes (`[]`, append, boucles `for`)
        
    - Les classes (`class`, `__init__`, `self`)
        
    - Les m√©thodes (`def method(self, param):`)
        
- **Test rapide** : Cr√©e un fichier `test_poo.py` et code :
```python
class Point:
    def __init__(self, x, y):
        self.x = x
        self.y = y
    
    def distance(self):
        return (self.x**2 + self.y**2)**0.5

p = Point(3, 4)
print(p.distance())  # Doit afficher 5.0
```

**‚ú® Validation** : Si ton code fonctionne, tu es pr√™t pour la suite !

---

## ‚úÖ √âtape 1.2 : Comprendre le Perceptron (1h)

**Objectif** : Saisir la logique math√©matique d'un neurone

**Actions concr√®tes :**

1. **Va sur [https://playground.tensorflow.org**](https://playground.tensorflow.org%2A%2A/)[playground.tensorflow+1](https://playground.tensorflow.org/)
    
    - Clique sur "Play" et regarde le r√©seau s'entra√Æner
        
    - Observe les **poids** (lignes bleues/oranges) et les **neurones** (cercles)
        
    - Change le nombre de neurones/couches et observe
        
2. **Note ces 3 formules cl√©s** (c'est TOUTE la magie du perceptron) :
    
    - **Produit scalaire** : z=w1‚ãÖx1+w2‚ãÖx2+...+wn‚ãÖxnz = w_1 \cdot x_1 + w_2 \cdot x_2 + ... + w_n \cdot x_nz=w1‚ãÖx1+w2‚ãÖx2+...+wn‚ãÖxn
        
    - **Ajout du biais** : z=z+bz = z + bz=z+b
        
    - **Activation** : sortie=f(z)sortie = f(z)sortie=f(z) o√π fff peut √™tre sigmoid, ReLU, etc.[[tensorflow](https://www.tensorflow.org/guide/core/mlp_core)]‚Äã
        
3. **Exemple concret √† la main** :
    
    - Inputs : `[1, 2]`
        
    - Poids : `[0.5, -0.3]`
        
    - Biais : `0.1`
        
    - Calcul : z=1√ó0.5+2√ó(‚àí0.3)+0.1=0.5‚àí0.6+0.1=0.0z = 1 \times 0.5 + 2 \times (-0.3) + 0.1 = 0.5 - 0.6 + 0.1 = 0.0z=1√ó0.5+2√ó(‚àí0.3)+0.1=0.5‚àí0.6+0.1=0.0
        
    - Activation ReLU : max(0,0.0)=0.0max(0, 0.0) = 0.0max(0,0.0)=0.0
        

**‚ú® Validation** : Tu dois pouvoir expliquer "un neurone = produit scalaire + biais + activation"

---

## ‚úÖ √âtape 1.3 : Coder `neuron.py` (1h30)

**Objectif** : Impl√©menter ta premi√®re classe `Neuron`

**Architecture attendue :**

```python
class Neuron:
    def __init__(self, weights, bias):
        # Stocker poids et biais
        pass
    
    def forward(self, inputs):
        # 1. Produit scalaire inputs √ó weights
        # 2. Ajouter le biais
        # 3. Retourner le r√©sultat (sans activation pour l'instant)
        pass
```

**Actions concr√®tes :**

1. Cr√©e le fichier `neuron.py` dans VS Code
    
2. **Code le `__init__`** :
    
    - Stocke `self.weights` (liste)
        
    - Stocke `self.bias` (nombre)
        
3. **Code la m√©thode `forward`** :
    
    - Fais une boucle `for` pour calculer le produit scalaire
        
    - Ajoute le biais
        
    - Retourne la somme
        
4. **Test imm√©diat** :
```python
# √Ä la fin de neuron.py
if __name__ == "__main__":
    n = Neuron(weights=[0.5, -0.3], bias=0.1)
    result = n.forward([1, 2])
    print(f"R√©sultat: {result}")  # Doit afficher 0.0
```

**‚ú® Validation** : Ton test affiche `0.0` ? Bravo, ton neurone calcule correctement !

---

## ‚úÖ √âtape 1.4 : Ajouter les activations (1h)

**Objectif** : Rendre ton neurone "intelligent" avec des fonctions d'activation

**Fonctions √† coder (dans `neuron.py` ou un fichier s√©par√© `activations.py`) :**

```python
def identity(x):
    return x

def heaviside(x):
    return 1 if x >= 0 else 0

def sigmoid(x):
    return 1 / (1 + (2.718281828 ** (-x)))  # e ‚âà 2.718...

def relu(x):
    return max(0, x)
```

**Modifier la classe `Neuron` :**

```python
class Neuron:
    def __init__(self, weights, bias, activation=identity):
        self.weights = weights
        self.bias = bias
        self.activation = activation
    
    def forward(self, inputs):
        z = # ... ton calcul pr√©c√©dent
        return self.activation(z)  # <-- AJOUT ICI
```

**Test avec diff√©rentes activations :**

```python
n_sigmoid = Neuron([0.5, -0.3], 0.1, activation=sigmoid)
n_relu = Neuron([0.5, -0.3], 0.1, activation=relu)
print(n_sigmoid.forward([1, 2]))  # ~0.5
print(n_relu.forward([1, 2]))     # 0.0
```

**‚ú® Validation** : Tu peux changer l'activation et voir des r√©sultats diff√©rents !

---

## ‚è±Ô∏è JOUR 2 : Layer + Tests (3-4h)

## ‚úÖ √âtape 2.1 : Comprendre une couche (30 min)

**Concept cl√© :** Une couche (`Layer`) = plusieurs neurones qui re√ßoivent les M√äMES inputs[[tensorflow](https://www.tensorflow.org/guide/core/mlp_core)]‚Äã

**Sch√©ma mental :**

```text
Inputs: [x1, x2, x3]
         ‚Üì   ‚Üì   ‚Üì
       ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
       ‚Üì           ‚Üì     ‚Üì
    Neuron1    Neuron2  Neuron3  ‚Üê 3 neurones dans la couche
       ‚Üì           ‚Üì     ‚Üì
    [out1]      [out2] [out3]    ‚Üê Sortie = liste de 3 valeurs
```

**Sur le playground TensorFlow :**

- Une colonne de cercles = 1 layer
    
- Chaque cercle = 1 neurone
    
- Tous les cercles d'une colonne re√ßoivent les m√™mes inputs[playground.tensorflow+1](https://playground.tensorflow.org/)
    

---

## ‚úÖ √âtape 2.2 : Coder `layer.py` (1h30)

**Architecture attendue :**

```python
from neuron import Neuron

class Layer:
    def __init__(self, num_neurons, num_inputs, activation):
        # Cr√©er une liste de neurones
        # Tous avec le m√™me nombre d'inputs
        pass
    
    def forward(self, inputs):
        # Appeler forward() sur chaque neurone
        # Retourner une liste des sorties
        pass
```

**Actions concr√®tes :**

1. Cr√©e `layer.py`
    
2. **Dans `__init__` :**
    
    - Cr√©e une liste vide `self.neurons = []`
        
    - Boucle `for` pour cr√©er `num_neurons` neurones
        
    - Chaque neurone a `num_inputs` poids (initialise avec des 0.5 pour tester)
        
3. **Dans `forward` :**
    
    - Cr√©e une liste vide `outputs = []`
        
    - Pour chaque neurone, appelle `neuron.forward(inputs)`
        
    - Ajoute le r√©sultat √† `outputs`
        
    - Retourne `outputs`
        

**Test imm√©diat :**

```python
if __name__ == "__main__":
    from activations import relu
    layer = Layer(num_neurons=3, num_inputs=2, activation=relu)
    result = layer.forward([1.0, 2.0])
    print(f"Sorties de la couche: {result}")  # Liste de 3 valeurs

```

**‚ú® Validation** : Tu dois voir `[valeur1, valeur2, valeur3]`

---

## ‚úÖ √âtape 2.3 : Initialisation intelligente des poids (1h)

**Probl√®me :** Tous les poids √† 0.5, c'est pas terrible

**Solutions √† impl√©menter :**

1. **Poids al√©atoires :**

```python
import random
random.seed(42)  # Pour reproductibilit√©
weight = random.uniform(-1, 1)  # Entre -1 et 1

```
   
2. **Modifier `Neuron.__init__` pour accepter `None` :**
   
```python
def __init__(self, weights=None, num_inputs=None, bias=0, activation=identity):
    if weights is None:
        # G√©n√©rer des poids al√©atoires
        self.weights = [random.uniform(-1, 1) for _ in range(num_inputs)]
    else:
        self.weights = weights
    # ...

```

**Test :**

python

`n = Neuron(num_inputs=3)  # Poids auto-g√©n√©r√©s print(n.weights)  # Doit afficher 3 valeurs al√©atoires`

**‚ú® Validation** : Chaque cr√©ation de neurone donne des poids diff√©rents

---

## ‚úÖ √âtape 2.4 : Tests unitaires basiques (1h)

**Objectif :** V√©rifier que ton code marche avec des cas simples

**Cr√©e `test_manual.py` :**

```python
from neuron import Neuron
from layer import Layer
from activations import identity, relu

# Test 1: Neurone avec poids fix√©s
n = Neuron(weights=[1, 1], bias=0, activation=identity)
assert n.forward([2, 3]) == 5, "Erreur calcul neurone"
print("‚úÖ Test neurone OK")

# Test 2: Couche avec 2 neurones
# ...

```

**‚ú® Validation** : Tous tes `print("‚úÖ ...")` s'affichent sans erreur

---

## ‚è±Ô∏è JOUR 3 : Network + Integration (4-5h)

## ‚úÖ √âtape 3.1 : Comprendre le r√©seau multi-couches (30 min)

**Concept cl√© :** Les sorties d'une couche = inputs de la couche suivante[[tensorflow](https://www.tensorflow.org/guide/core/mlp_core)]‚Äã

**Sch√©ma :**

```text
Input Layer    Hidden Layer    Output Layer
[x1, x2, x3] ‚Üí [h1, h2, h3, h4] ‚Üí [y1]
              ‚Üë                   ‚Üë
          sorties Layer1 =   sorties Layer2 =
          inputs Layer2      sortie finale

```

**Sur le playground :**

- Observe comment les valeurs "coulent" de gauche √† droite
    
- C'est la **forward pass** (passage avant)[[youtube](https://www.youtube.com/watch?v=wmqTVC17KV4)]‚Äã
    

---

## ‚úÖ √âtape 3.2 : Coder `network.py` - Version simple (2h)

**Architecture attendue :**

```python
from layer import Layer

class Network:
    def __init__(self, layer_sizes, activations):
        # layer_sizes = [3, 4, 1] ‚Üí 3 inputs, 4 hidden, 1 output
        # activations = [relu, sigmoid]
        self.layers = []
        # Cr√©er les couches
        pass
    
    def forward(self, inputs):
        # Passer inputs dans chaque couche successivement
        pass

```

**Actions concr√®tes :**

1. **Dans `__init__` :**

```python
for i in range(len(layer_sizes) - 1):
    num_inputs = layer_sizes[i]
    num_neurons = layer_sizes[i + 1]
    activation = activations[i]
    layer = Layer(num_neurons, num_inputs, activation)
    self.layers.append(layer)

```

2. **Dans `forward` :**

```python
current = inputs
for layer in self.layers:
    current = layer.forward(current)
return current

```

**Test :**

```python
if __name__ == "__main__":
    from activations import relu, sigmoid
    net = Network(
        layer_sizes=[2, 3, 1],
        activations=[relu, sigmoid]
    )
    result = net.forward([1.0, 2.0])
    print(f"Sortie du r√©seau: {result}")  # Liste avec 1 valeur

```

**‚ú® Validation** : Tu obtiens une sortie (m√™me si elle est bizarre pour l'instant)

---

## ‚úÖ √âtape 3.3 : Int√©gration avec le `main.py` fourni (1h30)

**Objectif :** Faire tourner le code de ton prof

**Actions :**

1. Ouvre le PDF de ton √©nonc√© (dans l'espace)
    
2. Copie le `main.py` fourni
    
3. V√©rifie que ton code est compatible :
    
    - Noms de classes corrects ?
        
    - Noms de m√©thodes corrects ?
        
    - Ordre des param√®tres correct ?
        
4. **Lance :** `python main.py`
    
5. **Debug les erreurs** (c'est normal d'en avoir !)
    

**Erreurs courantes :**

- `AttributeError` ‚Üí M√©thode manquante
    
- `TypeError` ‚Üí Mauvais nombre de param√®tres
    
- `ValueError` ‚Üí Mauvaise dimension de donn√©es
    

**‚ú® Validation** : Le `main.py` s'ex√©cute jusqu'au bout sans crash

---

## ‚úÖ √âtape 3.4 : V√©rifications et ajustements (1h)

**Checklist finale :**

-  Le r√©seau accepte n'importe quelle architecture (modifie `layer_sizes` dans `main.py`)
    
-  Les activations sont bien appliqu√©es
    
-  Les poids sont diff√©rents √† chaque ex√©cution (sauf si `random.seed()`)
    
-  Le code est comment√© (au moins les m√©thodes principales)
    
-  Chaque fichier a un `if __name__ == "__main__":` avec un test
    

**‚ú® Validation** : Ton code est propre et fonctionne

---

## ‚è±Ô∏è JOUR 4 : Finitions + Documentation (2-3h)

## ‚úÖ √âtape 4.1 : Cas d'usage classiques (1h)

**Impl√©mente 2 exemples :**

1. **Perceptron AND (porte logique) :**

```python
# Poids fix√©s pour r√©soudre AND
n = Neuron(weights=[1, 1], bias=-1.5, activation=heaviside)
print(n.forward([0, 0]))  # 0
print(n.forward([1, 1]))  # 1
```

2. **Impossible XOR avec 1 neurone :**
    
    - Montre qu'un seul neurone ne peut pas r√©soudre XOR
        
    - Explique pourquoi il faut une couche cach√©e
        

---

## ‚úÖ √âtape 4.2 : M√©thode `summary()` (optionnel, 30 min)

**Afficher l'architecture du r√©seau :**

```python
def summary(self):
    print("=" * 50)
    print("ARCHITECTURE DU R√âSEAU")
    print("=" * 50)
    for i, layer in enumerate(self.layers):
        print(f"Layer {i+1}: {len(layer.neurons)} neurones")
    print("=" * 50)
```

---

## ‚úÖ √âtape 4.3 : README.md (30 min)

**R√©dige un fichier README simple :**

```text
# Toyceptron - Perceptron Multi-Couches

## Description
Impl√©mentation d'un r√©seau de neurones en Python pur (sans librairies).

## Utilisation
```bash
python main.py
```

## Architecture

- `neuron.py` : Classe Neuron
    
- `layer.py` : Classe Layer
    
- `network.py` : Classe Network
    
- `activations.py` : Fonctions d'activation
    

## Auteur

[Ton nom] - [Date]


***

### ‚úÖ √âtape 4.4 : Relecture et test final (1h)
**Actions :**
1. Relis ton code du d√©but √† la fin
2. Teste tous les cas d'usage
3. V√©rifie que le `main.py` fonctionne
4. Pr√©pare ta pr√©sentation (si n√©cessaire)

**‚ú® Validation finale** : Ton projet est pr√™t √† √™tre rendu ! üéâ

***

## üéØ R√âSUM√â ULTRA-SYNTH√âTIQUE

| Jour | Focus | Livrable |
|------|-------|----------|
| **J1** | Neuron + Activations | `neuron.py` fonctionnel |
| **J2** | Layer + Tests | `layer.py` fonctionnel |
| **J3** | Network + Integration | `network.py` + `main.py` OK |
| **J4** | Finitions + Doc | Projet complet |

***

## üö® PI√àGES √Ä √âVITER

1. **Dimensions incompatibles** : V√©rifie toujours que `len(inputs) == len(weights)`
2. **Oublier `self`** : En POO, n'oublie jamais `self.` devant les attributs
3. **Boucles infinies** : Teste avec de petites architectures d'abord
4. **Confusion liste/scalaire** : `forward()` d'un neurone retourne un nombre, pas une liste

***

## üí° ASTUCES DE DERNI√àRE MINUTE

- **Bloqu√© sur un bug ?** ‚Üí Utilise `print()` partout pour voir les valeurs
- **Code qui plante ?** ‚Üí V√©rifie les dimensions avec `print(len(...))` 
- **R√©sultat bizarre ?** ‚Üí C'est normal sans entra√Ænement, les poids sont al√©atoires
- **Manque de temps ?** ‚Üí Concentre-toi sur Neuron ‚Üí Layer ‚Üí Network, le reste est bonus

***

## üìö RESSOURCES RAPIDES

- **Refresh Python POO** : https://learnxinyminutes.com/python/
- **Visualiser** : https://playground.tensorflow.org
- **R√©f√©rence technique** : PDF dans l'espace

***
