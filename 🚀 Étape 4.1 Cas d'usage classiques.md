<img src="https://r2cdn.perplexity.ai/pplx-full-logo-primary-dark%402x.png" style="height:64px;margin-right:32px"/>

# Sprint Toyceptron - JOUR 4 d√©marr√©

√âtape 4.1 : Cas d'usage classiques
Go ! üöÄ

Excellent ! üöÄ On attaque l'**√âtape 4.1 : Cas d'usage classiques** ‚Äî tu arrives sur la ligne droite finale du projet Toyceptron !

## Objectif de l'√©tape

D√©montrer que ton r√©seau fonctionne vraiment en impl√©mentant **2 exemples classiques** :

1. **Perceptron AND** (porte logique) avec poids fix√©s
2. **D√©monstration XOR impossible** avec un seul neurone

[^1]

***

## 1. Perceptron AND ‚Äî Poids fix√©s

### Concept math√©matique

La porte AND retourne 1 seulement si **toutes les entr√©es valent 1** :


| x‚ÇÅ | x‚ÇÇ | Sortie |
| :-- | :-- | :-- |
| 0 | 0 | 0 |
| 0 | 1 | 0 |
| 1 | 0 | 0 |
| 1 | 1 | 1 |

**Comment un neurone peut apprendre √ßa ?**

- Produit scalaire : $z = w_1 x_1 + w_2 x_2 + b$
- Activation : Heaviside (seuil) ‚Üí retourne 1 si $z \geq 0$, sinon 0

**Poids magiques** :

- $w_1 = 1, w_2 = 1, b = -1.5$
- Si $x_1 = 0, x_2 = 0$ ‚Üí $z = 0 + 0 - 1.5 = -1.5$ ‚Üí sortie = **0**
- Si $x_1 = 1, x_2 = 1$ ‚Üí $z = 1 + 1 - 1.5 = 0.5$ ‚Üí sortie = **1**

***

### Code √† impl√©menter

Cr√©e un fichier **`test_and.py`** :

```python
from neuron import Neuron
from activations import heaviside

# Neurone configur√© pour r√©soudre AND
neuron_and = Neuron(
    weights=[1, 1],
    bias=-1.5,
    activation=heaviside
)

# Table de v√©rit√©
print("=" * 40)
print("PERCEPTRON AND - Table de v√©rit√©")
print("=" * 40)

tests = [
    (0, 0, 0),
    (0, 1, 0),
    (1, 0, 0),
    (1, 1, 1)
]

for x1, x2, expected in tests:
    result = neuron_and.forward([x1, x2])
    status = "‚úì" if result == expected else "‚úó"
    print(f"{status} AND({x1}, {x2}) = {result} (attendu: {expected})")

print("=" * 40)
```

**Validation attendue** :

```
‚úì AND(0, 0) = 0 (attendu: 0)
‚úì AND(0, 1) = 0 (attendu: 0)
‚úì AND(1, 0) = 0 (attendu: 0)
‚úì AND(1, 1) = 1 (attendu: 1)
```


***

## 2. Impossibilit√© de XOR avec un seul neurone

### Pourquoi XOR est impossible ?

La porte XOR :


| x‚ÇÅ | x‚ÇÇ | Sortie |
| :-- | :-- | :-- |
| 0 | 0 | 0 |
| 0 | 1 | 1 |
| 1 | 0 | 1 |
| 1 | 1 | 0 |

**Probl√®me** : Ces points ne sont **pas lin√©airement s√©parables**. Aucune ligne droite ne peut s√©parer les 0 des 1.[^1]

Sur le playground TensorFlow, si tu essaies avec **0 hidden layer**, le r√©seau √©choue toujours sur XOR. Il faut **au moins 1 couche cach√©e** pour cr√©er une fronti√®re de d√©cision non-lin√©aire.

***

### D√©monstration code

Cr√©e **`test_xor_impossible.py`** :

```python
from neuron import Neuron
from activations import heaviside

print("=" * 50)
print("D√âMONSTRATION : XOR IMPOSSIBLE avec 1 neurone")
print("=" * 50)

# Essayons plusieurs configurations de poids
configs = [
    {"weights": [1, 1], "bias": -0.5, "name": "Conf 1"},
    {"weights": [1, -1], "bias": 0, "name": "Conf 2"},
    {"weights": [-1, 1], "bias": 0, "name": "Conf 3"},
]

xor_truth_table = [
    (0, 0, 0),
    (0, 1, 1),
    (1, 0, 1),
    (1, 1, 0)
]

for config in configs:
    n = Neuron(
        weights=config["weights"],
        bias=config["bias"],
        activation=heaviside
    )
    
    print(f"\n{config['name']} ‚Üí w={config['weights']}, b={config['bias']}")
    errors = 0
    
    for x1, x2, expected in xor_truth_table:
        result = n.forward([x1, x2])
        status = "‚úì" if result == expected else "‚úó"
        if result != expected:
            errors += 1
        print(f"  {status} XOR({x1}, {x2}) = {result} (attendu: {expected})")
    
    print(f"  ‚Üí Erreurs : {errors}/4")

print("\n" + "=" * 50)
print("CONCLUSION : Impossible de r√©soudre XOR avec")
print("un seul neurone. Il faut une couche cach√©e !")
print("=" * 50)
```

**Sortie attendue** :
Toutes les configurations auront **au moins 1 erreur**.[^1]

***

## 3. BONUS : XOR avec couche cach√©e

Si tu veux aller plus loin, d√©montre que **XOR devient possible avec 2 couches** :

```python
from network import Network
from activations import heaviside

# Architecture : 2 inputs ‚Üí 2 hidden ‚Üí 1 output
net = Network(
    layer_sizes=[2, 2, 1],
    activations=[heaviside, heaviside]
)

# Avec des poids al√©atoires, ce ne sera pas parfait
# Mais tu peux montrer que l'architecture PERMET de r√©soudre XOR
# (m√™me si sans entra√Ænement, les r√©sultats sont incorrects)

print("\nXOR avec couche cach√©e (poids non entra√Æn√©s) :")
for x1, x2, expected in [(0,0,0), (0,1,1), (1,0,1), (1,1,0)]:
    result = net.forward([x1, x2])[^0]
    print(f"XOR({x1}, {x2}) = {result:.2f} (attendu: {expected})")

print("\n‚ö†Ô∏è Sans entra√Ænement (backpropagation), les poids sont al√©atoires.")
print("Mais l'ARCHITECTURE permet math√©matiquement de r√©soudre XOR !")
```


***

## Checklist de validation

- [ ] `test_and.py` affiche 4/4 r√©ussites ‚úì
- [ ] `test_xor_impossible.py` montre que XOR √©choue avec 1 neurone
- [ ] Tu comprends **pourquoi** XOR n√©cessite une couche cach√©e
- [ ] Tu peux expliquer le lien avec le playground TensorFlow

***

## Prochaine √©tape

Une fois valid√© :

- **√âtape 4.2** : M√©thode `summary()` (optionnel, 30 min)
- **√âtape 4.3** : R√©daction du README
- **√âtape 4.4** : Relecture finale et tests

Tu es **quasi au bout** ! üéØ Lance les tests et dis-moi ce que √ßa donne. üí™

<div align="center">‚ÅÇ</div>

[^1]: ROADMAP-TOYCEPTRON-Mode-Sprint-3-4-jours.md

