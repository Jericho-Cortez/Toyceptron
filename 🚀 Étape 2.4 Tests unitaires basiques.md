<img src="https://r2cdn.perplexity.ai/pplx-full-logo-primary-dark%402x.png" style="height:64px;margin-right:32px"/>

## ğŸ“š RÃ‰CAPITULATIF COMPLET - Ã‰TAPE 2.4 : Tests unitaires basiques

Je te refais **tout le parcours de l'Ã©tape 2.4** avec explications dÃ©taillÃ©es.[^1]

***

## ğŸ¯ Objectif de l'Ã©tape 2.4

**CrÃ©er des tests robustes avec des calculs vÃ©rifiables manuellement** pour s'assurer que `Neuron` et `Layer` fonctionnent parfaitement avant d'attaquer `Network.py`.[^1]

### Pourquoi cette Ã©tape est cruciale ?

- **SÃ©curitÃ©** : Si un bug apparaÃ®t plus tard dans `Network`, tu sauras que le problÃ¨me n'est PAS dans `Neuron` ou `Layer`
- **ComprÃ©hension** : En vÃ©rifiant les calculs Ã  la main, tu **comprends vraiment** ce que fait ton code
- **Professionnalisme** : Les tests unitaires sont la base de tout projet sÃ©rieux[^1]

***

## ğŸ“ Le fichier `test_manual.py` - Section par section

### ğŸ”´ Test 1 : Neurone avec poids fixes

```python
print("\n[Test 1] Neurone avec poids fixes - Calcul vÃ©rifiable")
n = Neuron(weights=[1, 1], bias=0, activation=identity)
result = n.forward([2, 3])

print(f"Calcul manuel : 1*2 + 1*3 + 0 = 5")
assert result == 5, f"âŒ ERREUR : attendu 5, obtenu {result}"
print("âœ… Test 1 rÃ©ussi : Calcul correct")
```


#### ğŸ“– Explication dÃ©taillÃ©e

**Ce qu'on teste** : Le calcul du produit scalaire + biais

**MathÃ©matiques** :

- Formule : $z = w_1 \times x_1 + w_2 \times x_2 + b$
- Avec nos valeurs : $z = 1 \times 2 + 1 \times 3 + 0 = 5$
- Activation `identity` â†’ sortie = $z$ (pas de transformation)

**Pourquoi des poids fixes ?**

- Avec des poids alÃ©atoires, tu ne peux pas vÃ©rifier le rÃ©sultat Ã  la main
- Ici, tu sais **exactement** ce que tu dois obtenir : 5[^1]

**Ce que Ã§a prouve** :
âœ… Ton produit scalaire fonctionne
âœ… L'addition du biais fonctionne
âœ… La mÃ©thode `forward()` retourne le bon rÃ©sultat

***

### ğŸ”´ Test 2 : Neurone avec ReLU

```python
print("\n[Test 2] Neurone avec ReLU - Valeur nÃ©gative")
n_relu = Neuron(weights=[1, -2], bias=-1, activation=relu)
result_relu = n_relu.forward([1, 2])

print(f"Calcul manuel : 1*1 + (-2)*2 + (-1) = 1 - 4 - 1 = -4")
print(f"AprÃ¨s ReLU : max(0, -4) = 0")
assert result_relu == 0
print("âœ… Test 2 rÃ©ussi : ReLU fonctionne correctement")
```


#### ğŸ“– Explication dÃ©taillÃ©e

**Ce qu'on teste** : La fonction d'activation ReLU sur une valeur **nÃ©gative**

**MathÃ©matiques** :

1. Produit scalaire : $z = 1 \times 1 + (-2) \times 2 + (-1) = -4$
2. ReLU : $\text{relu}(-4) = \max(0, -4) = 0$

**Formule ReLU** :

```python
def relu(x):
    return max(0, x)
```

**Pourquoi tester une valeur nÃ©gative ?**

- ReLU est intÃ©ressant quand $z < 0$ (il "coupe" les valeurs nÃ©gatives)
- Si $z > 0$, ReLU retourne simplement $z$ (pas de transformation)[^1]

**Ce que Ã§a prouve** :
âœ… L'activation est bien appliquÃ©e aprÃ¨s le calcul
âœ… ReLU bloque correctement les valeurs nÃ©gatives

***

### ğŸ”´ Test 3 : Porte logique AND

```python
print("\n[Test 3] Porte logique AND - Application concrÃ¨te")
n_and = Neuron(weights=[1, 1], bias=-1.5, activation=heaviside)

test_cases = [
    ([0, 0], 0),  # 0 + 0 - 1.5 = -1.5 â†’ 0
    ([1, 0], 0),  # 1 + 0 - 1.5 = -0.5 â†’ 0
    ([0, 1], 0),  # 0 + 1 - 1.5 = -0.5 â†’ 0
    ([1, 1], 1),  # 1 + 1 - 1.5 = 0.5 â†’ 1
]

for inputs, expected in test_cases:
    result = n_and.forward(inputs)
    assert result == expected
```


#### ğŸ“– Explication dÃ©taillÃ©e

**Ce qu'on teste** : Un neurone peut **rÃ©soudre un problÃ¨me logique** !

**Table de vÃ©ritÃ© AND** :


| A | B | A AND B |
| :-- | :-- | :-- |
| 0 | 0 | 0 |
| 0 | 1 | 0 |
| 1 | 0 | 0 |
| 1 | 1 | 1 |

**Comment Ã§a marche ?**

Pour chaque cas, on calcule $z = 1 \times A + 1 \times B - 1.5$ :

1. **** : $z = 0 + 0 - 1.5 = -1.5$ â†’ `heaviside(-1.5)` = 0 âœ…
2. **** : $z = 1 + 0 - 1.5 = -0.5$ â†’ `heaviside(-0.5)` = 0 âœ…[^1]
3. **** : $z = 0 + 1 - 1.5 = -0.5$ â†’ `heaviside(-0.5)` = 0 âœ…[^1]
4. **** : $z = 1 + 1 - 1.5 = 0.5$ â†’ `heaviside(0.5)` = 1 âœ…[^1]

**Fonction Heaviside** (seuil) :

```python
def heaviside(x):
    return 1 if x >= 0 else 0
```

**Visualisation sur TensorFlow Playground** :
Va sur [playground.tensorflow.org](https://playground.tensorflow.org) :

- Dataset : SÃ©lectionne "Circle" ou crÃ©e un pattern simple
- 2 inputs, 1 neurone, activation "ReLU" ou "Linear"
- Un seul neurone peut tracer **une ligne de sÃ©paration**[^1]

**Ce que Ã§a prouve** :
âœ… Un neurone = un **classificateur linÃ©aire**
âœ… Il peut rÃ©soudre AND (sÃ©parable linÃ©airement)
âœ… Mais il **ne peut PAS** rÃ©soudre XOR (non-sÃ©parable linÃ©airement) â†’ besoin d'une couche cachÃ©e[^1]

***

### ğŸ”´ Test 4 : Layer avec calcul manuel

```python
print("\n[Test 4] Layer - VÃ©rification d'une couche complÃ¨te")
random.seed(999)
layer_test = Layer(num_neurons=2, num_inputs=2, activation=identity)

print(f"Neurone 1 : weights={layer_test.neurons[^0].weights}")
print(f"Neurone 2 : weights={layer_test.neurons[^1].weights}")

inputs_test = [1.0, 0.5]
result_layer = layer_test.forward(inputs_test)

assert len(result_layer) == 2
assert isinstance(result_layer, list)
print("âœ… Test 4 rÃ©ussi : La couche fonctionne correctement")
```


#### ğŸ“– Explication dÃ©taillÃ©e

**Ce qu'on teste** : Une `Layer` retourne bien une **liste de sorties**

**Architecture** :

```
inputs [1.0, 0.5]
       â†“
   â”Œâ”€â”€â”€â”´â”€â”€â”€â”
   â†“       â†“
Neurone1  Neurone2  (mÃªme inputs pour les 2)
   â†“       â†“
output1  output2
   â””â”€â”€â”€â”¬â”€â”€â”€â”˜
       â†“
[output1, output2]
```

**Avec seed=999**, on fixe les poids pour la reproductibilitÃ© :

- Neurone 1 : `weights=[0.562, -0.839]`, `bias=0.745`
- Neurone 2 : `weights=[0.147, -0.018]`, `bias=-0.736`

**Calculs manuels** :

1. Neurone 1 : $z_1 = 0.562 \times 1.0 + (-0.839) \times 0.5 + 0.745 = 0.887$
2. Neurone 2 : $z_2 = 0.147 \times 1.0 + (-0.018) \times 0.5 + (-0.736) = -0.597$

**Sortie** : `[0.887, -0.597]` â†’ Liste de 2 valeurs (1 par neurone)

**Ce que Ã§a prouve** :
âœ… `Layer.forward()` retourne bien une liste
âœ… Chaque neurone reÃ§oit les **mÃªmes inputs**[^1]
âœ… La taille de la sortie = nombre de neurones

***

### ğŸ”´ Test 5 : ReproductibilitÃ© avec seed

```python
print("\n[Test 5] ReproductibilitÃ© - MÃªme seed = MÃªmes poids")
random.seed(42)
layer1 = Layer(num_neurons=3, num_inputs=2, activation=identity)
result1 = layer1.forward([1.0, 2.0])

random.seed(42)  # Reset avec le MÃŠME seed
layer2 = Layer(num_neurons=3, num_inputs=2, activation=identity)
result2 = layer2.forward([1.0, 2.0])

assert result1 == result2
print("âœ… ReproductibilitÃ© garantie")
```


#### ğŸ“– Explication dÃ©taillÃ©e

**Ce qu'on teste** : Avec le mÃªme `seed`, on obtient **exactement les mÃªmes rÃ©sultats**

**Pourquoi c'est important ?**
Imagine ce scÃ©nario :

1. Tu lances ton code â†’ bug bizarre
2. Tu le relances â†’ le bug **a disparu** (poids diffÃ©rents !)
3. Impossible de dÃ©boguer ğŸ˜±

**Solution** : `random.seed(42)` fixe l'alÃ©atoire

**Avec seed=42** :

```python
Layer 1 poids : [[0.278, -0.949], [-0.553, 0.472], [0.784, -0.826]]
Layer 2 poids : [[0.278, -0.949], [-0.553, 0.472], [0.784, -0.826]]
```

**Exactement identiques !**[^1]

**RÃ©sultats** :

```
RÃ©sultat 1 : [-2.071, 0.745, -1.024]
RÃ©sultat 2 : [-2.071, 0.745, -1.024]
```

**Ce que Ã§a prouve** :
âœ… Le `seed` contrÃ´le bien la gÃ©nÃ©ration alÃ©atoire
âœ… Tu peux **reproduire n'importe quel bug** pour le dÃ©boguer[^1]

***

### ğŸ”´ Test 6 : Gestion d'erreur

```python
print("\n[Test 6] Gestion des erreurs - ParamÃ¨tres invalides")
try:
    n_error = Neuron(weights=None, num_inputs=None)
    print("âŒ Ã‰CHEC : L'erreur n'a pas Ã©tÃ© levÃ©e")
except ValueError as e:
    print(f"âœ… Erreur correctement levÃ©e : {e}")
```


#### ğŸ“– Explication dÃ©taillÃ©e

**Ce qu'on teste** : Ton code refuse les **paramÃ¨tres invalides**

**Cas d'erreur** :

```python
Neuron(weights=None, num_inputs=None)
```

â†’ Si `weights=None`, il faut **obligatoirement** `num_inputs` pour gÃ©nÃ©rer les poids !

**Code de protection dans `Neuron.__init__()`** :

```python
if weights is None:
    if num_inputs is None:
        raise ValueError("Si weights=None, num_inputs doit Ãªtre fourni")
```

**RÃ©sultat attendu** :

```
âœ… Erreur correctement levÃ©e : Si weights=None, num_inputs doit Ãªtre fourni
```

**Ce que Ã§a prouve** :
âœ… Ton code ne plante pas silencieusement
âœ… Il donne un **message d'erreur clair**[^1]
âœ… Protection contre les utilisations incorrectes

***

### ğŸ”´ Test 7 : Sigmoid avec z=0

```python
print("\n[Test 7] Activation Sigmoid - Valeur z=0")
n_sigmoid = Neuron(weights=[1, -1], bias=0, activation=sigmoid)
result_sigmoid = n_sigmoid.forward([2, 2])

print(f"Calcul : 1*2 + (-1)*2 + 0 = 0")
print(f"Sigmoid(0) devrait Ãªtre proche de 0.5")
assert abs(result_sigmoid - 0.5) < 0.01
print("âœ… Test 7 rÃ©ussi : Sigmoid fonctionne correctement")
```


#### ğŸ“– Explication dÃ©taillÃ©e

**Ce qu'on teste** : La fonction **sigmoid** au point $z = 0$

**MathÃ©matiques** :

1. Calcul : $z = 1 \times 2 + (-1) \times 2 + 0 = 0$
2. Sigmoid : $\sigma(0) = \frac{1}{1 + e^{0}} = \frac{1}{2} = 0.5$

**Formule sigmoid** :

```python
def sigmoid(x):
    return 1 / (1 + 2.718281828 ** (-x))
```

**PropriÃ©tÃ©s importantes de sigmoid** :

- $\sigma(0) = 0.5$ (point mÃ©dian)
- $\sigma(+\infty) \to 1$
- $\sigma(-\infty) \to 0$
- UtilisÃ©e pour **classification binaire** (sortie entre 0 et 1)

**Pourquoi tester z=0 ?**

- Point de rÃ©fÃ©rence simple
- Si $\sigma(0) \neq 0.5$, il y a un bug dans l'implÃ©mentation[^1]

**Ce que Ã§a prouve** :
âœ… Sigmoid est correctement implÃ©mentÃ©e
âœ… L'activation est appliquÃ©e aprÃ¨s le calcul

***

## ğŸ“ SynthÃ¨se : Ce que chaque test valide

| Test | Ce qu'il vÃ©rifie | Pourquoi c'est important |
| :-- | :-- | :-- |
| **Test 1** | Produit scalaire + biais | Base de tout neurone [^1] |
| **Test 2** | Activation ReLU | Gestion des valeurs nÃ©gatives |
| **Test 3** | Porte AND | Prouve qu'un neurone = classificateur linÃ©aire [^1] |
| **Test 4** | Layer complÃ¨te | VÃ©rification du flux de donnÃ©es |
| **Test 5** | ReproductibilitÃ© | Essentiel pour dÃ©boguer [^1] |
| **Test 6** | Gestion d'erreurs | Code robuste et sÃ»r |
| **Test 7** | Sigmoid | Validation d'une activation complexe |


***

## ğŸ§  Concepts clÃ©s Ã  retenir

### 1. Tests avec poids fixes vs alÃ©atoires

| Type | UtilitÃ© |
| :-- | :-- |
| **Poids fixes** | Calculs vÃ©rifiables manuellement (Test 1, 2, 3, 7) [^1] |
| **Poids alÃ©atoires** | VÃ©rifier la diversitÃ© (Test 4, 5) |

### 2. Porte logique AND â†’ Fondamental !

Un neurone peut tracer **une ligne de sÃ©paration** dans l'espace :

```
     1 |  âœ“
       |
A    0 | âœ—   âœ—
     ---+-------
       0   1   B
```

- Ligne de sÃ©paration : $A + B = 1.5$
- Au-dessus â†’ 1 (AND vrai)
- En dessous â†’ 0 (AND faux)

**Mais** : XOR nÃ©cessite **deux lignes** â†’ impossible avec 1 neurone â†’ besoin d'une couche cachÃ©e[^1]

### 3. random.seed() pour la reproductibilitÃ©

```python
random.seed(42)  # Fixe l'alÃ©atoire
n1 = Neuron(num_inputs=3)  # Poids : [0.278, -0.949, 0.784]

random.seed(42)  # MÃªme seed
n2 = Neuron(num_inputs=3)  # Poids : [0.278, -0.949, 0.784]
# Identiques !
```


***

## ğŸ”— Lien avec TensorFlow Playground

Va sur [playground.tensorflow.org](https://playground.tensorflow.org) et fais cette expÃ©rience :

### ExpÃ©rience 1 : Porte AND

1. Dataset : CrÃ©e un pattern avec 4 points (0,0), (1,0), (0,1), (1,1)
2. Colore (0,0), (1,0), (0,1) en bleu, (1,1) en orange
3. Architecture : 2 inputs, **0 hidden layers**, 1 output
4. Clique sur "Play" â†’ Le rÃ©seau **trouve la solution** (1 ligne de sÃ©paration)[^1]

### ExpÃ©rience 2 : XOR impossible

1. Dataset : SÃ©lectionne "XOR"
2. Architecture : 2 inputs, **0 hidden layers**, 1 output
3. Clique sur "Play" â†’ Le rÃ©seau **ne peut PAS converger** (impossible avec 1 ligne)
4. Ajoute **1 hidden layer avec 2 neurones** â†’ Maintenant Ã§a marche ![^1]

**Conclusion** : XOR nÃ©cessite au moins 2 neurones (2 lignes de sÃ©paration)

***

## âœ… Checklist finale de l'Ã©tape 2.4

Avant de passer au JOUR 3, tu dois pouvoir rÃ©pondre OUI Ã  tout :

- [ ] Je comprends pourquoi on utilise des **poids fixes** pour les tests[^1]
- [ ] Je sais calculer manuellement un forward pass (produit scalaire + biais + activation)
- [ ] Je comprends qu'un neurone = **une ligne de sÃ©paration**[^1]
- [ ] Je sais pourquoi AND fonctionne avec 1 neurone mais pas XOR[^1]
- [ ] Je comprends le rÃ´le de `random.seed()`[^1]
- [ ] Je sais que `Layer.forward()` retourne une **liste**[^1]
- [ ] Tous mes tests affichent `âœ…`

***

## ğŸš€ Prochaine Ã©tape : JOUR 3

Tu vas maintenant coder `Network.py` qui enchaÃ®ne plusieurs `Layer` :

```
Input â†’ Layer1 â†’ Layer2 â†’ Output
[x1,x2] â†’ [h1,h2,h3] â†’ [y1]
```

**Es-tu prÃªt Ã  attaquer le JOUR 3 ?** ğŸ’ªğŸ”¥

<div align="center">â‚</div>

[^1]: ROADMAP-TOYCEPTRON-Mode-Sprint-3-4-jours.md

