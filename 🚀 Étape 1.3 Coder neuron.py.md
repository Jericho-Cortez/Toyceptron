<img src="https://r2cdn.perplexity.ai/pplx-full-logo-primary-dark%402x.png" style="height:64px;margin-right:32px"/>

## üöÄ √âtape 1.3 : Coder `neuron.py` ‚Äî C'EST PARTI !

Parfait, on attaque le c≈ìur du projet ! Je vais t'aider √† cr√©er ton premier neurone artificiel en Python pur.[^1_1]

## üß† Rappel th√©orique ultra-rapide

Un neurone fait **3 choses** dans l'ordre  :[^1_1]

1. **Produit scalaire** : $z = w_1 \cdot x_1 + w_2 \cdot x_2 + ... + w_n \cdot x_n$
2. **Ajout du biais** : $z = z + b$
3. **Activation** : $\text{sortie} = f(z)$

**Exemple concret** : Si inputs = `[1, 2]`, poids = `[0.5, -0.3]`, biais = `0.1`[^1_1]

- Calcul : $1 \times 0.5 + 2 \times (-0.3) + 0.1 = 0.5 - 0.6 + 0.1 = 0.0$


## üìù CODE COMPLET DE `neuron.py`

```python
class Neuron:
    """
    Un neurone artificiel qui calcule :
    sortie = activation(somme_ponderee + biais)
    """
    
    def __init__(self, weights, bias):
        """
        Initialise le neurone avec ses param√®tres.
        
        Args:
            weights (list): Liste des poids [w1, w2, ..., wn]
            bias (float): Biais du neurone
        """
        self.weights = weights
        self.bias = bias
    
    def forward(self, inputs):
        """
        Calcule la sortie du neurone pour des inputs donn√©s.
        
        Args:
            inputs (list): Valeurs d'entr√©e [x1, x2, ..., xn]
        
        Returns:
            float: R√©sultat du calcul (sans activation pour l'instant)
        """
        # √âtape 1 : Produit scalaire inputs √ó weights
        z = 0.0
        for i in range(len(inputs)):
            z += inputs[i] * self.weights[i]
        
        # √âtape 2 : Ajouter le biais
        z += self.bias
        
        # √âtape 3 : Retourner le r√©sultat
        return z


# TEST IMM√âDIAT
if __name__ == "__main__":
    # Test avec l'exemple de la roadmap
    n = Neuron(weights=[0.5, -0.3], bias=0.1)
    result = n.forward([1, 2])
    print(f"R√©sultat: {result}")  # Doit afficher 0.0
    
    # Test suppl√©mentaire
    n2 = Neuron(weights=[1, 1], bias=0)
    result2 = n2.forward([2, 3])
    print(f"R√©sultat 2: {result2}")  # Doit afficher 5.0
```


## ‚úÖ EXPLICATION LIGNE PAR LIGNE

### Le constructeur `__init__`

```python
self.weights = weights  # Stocke la liste des poids
self.bias = bias        # Stocke le biais (un seul nombre)
```

Ces deux attributs sont la "m√©moire" du neurone.[^1_1]

### La m√©thode `forward`

```python
z = 0.0  # Initialise l'accumulateur √† z√©ro
for i in range(len(inputs)):
    z += inputs[i] * self.weights[i]  # Multiplie chaque entr√©e par son poids
```

Cette boucle fait le **produit scalaire** : $x_1 \times w_1 + x_2 \times w_2 + ...$[^1_1]

```python
z += self.bias  # Ajoute le biais au r√©sultat
return z        # Retourne la valeur finale
```


## üß™ VALIDATION ‚Äî Lance le test !

Ouvre un terminal et ex√©cute  :[^1_1]

```bash
python neuron.py
```

**Tu DOIS voir** :

```
R√©sultat: 0.0
R√©sultat 2: 5.0
```

Si √ßa marche : **BRAVO, ton neurone fonctionne !** üéâ

## üéØ PROCHAINE √âTAPE (√âtape 1.4)

Maintenant qu'on a le calcul de base, on va ajouter les **fonctions d'activation** pour rendre le neurone "intelligent"  :[^1_1]

- `identity` (identit√©)
- `heaviside` (seuil)
- `sigmoid` (sigmo√Øde)
- `relu` (ReLU)

**Tu veux qu'on attaque directement l'√âtape 1.4, ou tu veux d'abord tester ton `neuron.py` et me dire si √ßa marche ?** üí™

<div align="center">‚ÅÇ</div>

[^1_1]: ROADMAP-TOYCEPTRON-Mode-Sprint-3-4-jours.md

