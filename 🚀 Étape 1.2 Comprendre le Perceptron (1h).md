<img src="https://r2cdn.perplexity.ai/pplx-full-logo-primary-dark%402x.png" style="height:64px;margin-right:32px"/>

## üöÄ √âtape 1.2 : Comprendre le Perceptron (1h) - C'est parti !

Parfait, on d√©marre ! L'objectif de cette √©tape est de **saisir la logique math√©matique d'un neurone** avant de coder quoi que ce soit. C'est la fondation de tout ton projet Toyceptron.[^1_1]

***

## üß† Qu'est-ce qu'un Perceptron ?

Le perceptron est le **bloc √©l√©mentaire** d'un r√©seau de neurones. C'est un mod√®le math√©matique ultra-simple qui prend plusieurs entr√©es, les combine avec des poids, et produit une sortie.[^1_2][^1_3][^1_4][^1_5]

**Analogie** : Imagine un neurone comme un **d√©cideur** qui re√ßoit plusieurs signaux (tes inputs), leur attribue une importance diff√©rente (les poids), fait un total, ajoute un biais personnel, puis d√©cide quoi r√©pondre via une fonction d'activation.[^1_6]

***

## üéØ Les 3 op√©rations fondamentales d'un neurone

Voici **TOUTE la magie** d'un neurone en 3 √©tapes  :[^1_1]

### 1. **Produit scalaire** (weighted sum)

$$
z = w_1 \cdot x_1 + w_2 \cdot x_2 + ... + w_n \cdot x_n
$$

Tu multiplies chaque entr√©e $x_i$ par son poids $w_i$, puis tu additionnes tout.[^1_7][^1_3]

### 2. **Ajout du biais** (bias)

$$
z = z + b
$$

Le biais $b$ est une **constante** que tu ajoutes au r√©sultat. √áa permet au neurone de d√©caler sa d√©cision, comme un seuil d'activation personnalis√©.[^1_6][^1_1]

### 3. **Fonction d'activation**

$$
sortie = f(z)
$$

La fonction $f$ transforme le r√©sultat $z$ en sortie finale. Exemples : ReLU, sigmo√Øde, Heaviside, identit√©.[^1_8][^1_7][^1_1]

***

## üìê Exemple concret (calcul √† la main)

Prenons des valeurs num√©riques  :[^1_1]

**Donn√©es** :

- Inputs : $x_1 = 1$, $x_2 = 2$
- Poids : $w_1 = 0.5$, $w_2 = -0.3$
- Biais : $b = 0.1$
- Activation : **ReLU** (max(0, z))

**Calcul** :

1. Produit scalaire :

$$
z = (1 \times 0.5) + (2 \times -0.3) = 0.5 - 0.6 = -0.1
$$
2. Ajout du biais :

$$
z = -0.1 + 0.1 = 0.0
$$
3. Activation ReLU :

$$
sortie = \text{max}(0, 0.0) = 0.0
$$

**R√©sultat final** : 0.0[^1_1]

***

## üéÆ Visualisation interactive sur TensorFlow Playground

Va sur **https://playground.tensorflow.org**  :[^1_9][^1_10]

1. **Clique sur "Play"** et regarde le r√©seau s'entra√Æner en temps r√©el
2. **Observe** :
    - Les **lignes** entre neurones = les poids (bleu = positif, orange = n√©gatif)[^1_9]
    - Les **cercles** = les neurones
    - Le **fond color√©** = ce que le r√©seau pr√©dit pour chaque zone[^1_9]
3. **Exp√©rimente** : Change le nombre de neurones, de couches, les fonctions d'activation ‚Üí tu verras directement l'impact sur l'apprentissage[^1_1]

C'est **l'outil parfait** pour comprendre visuellement ce que tu vas coder.[^1_10][^1_1]

***

## ‚úÖ Validation de l'√©tape

Tu dois √™tre capable d'expliquer avec tes propres mots  :[^1_1]

- **Produit scalaire** : Combiner les inputs avec les poids
- **Biais** : D√©calage ajout√© √† la somme
- **Activation** : Transformation finale du r√©sultat

**Test mental** : Si je te donne des inputs [3, -1], des poids [0.2, 0.4], un biais de -0.1 et une activation ReLU, tu dois pouvoir calculer la sortie √† la main.

<details>
<summary>üí° Solution du test mental (clique pour r√©v√©ler)</summary>

1. Produit scalaire : \(z = 3 \times 0.2 + (-1) \times 0.4 = 0.6 - 0.4 = 0.2\)
2. Biais : \(z = 0.2 - 0.1 = 0.1\)
3. ReLU : \(\text{max}(0, 0.1) = 0.1\)

**R√©ponse** : 0.1
</details>

***

## üîú Prochaine √©tape

Une fois que tu ma√Ætrises ces 3 concepts, tu es **pr√™t pour l'√©tape 1.3** : coder ta classe `Neuron` en Python  ![^1_1]

**Timing** : Prends bien ton heure pour bien comprendre, n'h√©site pas √† jouer avec le Playground et √† refaire des calculs √† la main. La compr√©hension maintenant = du code facile apr√®s üí™

Tu as des questions sur les maths ou tu veux qu'on passe directement au code ?
<span style="display:none">[^1_11][^1_12][^1_13][^1_14][^1_15][^1_16][^1_17][^1_18][^1_19][^1_20]</span>

<div align="center">‚ÅÇ</div>

[^1_1]: ROADMAP-TOYCEPTRON-Mode-Sprint-3-4-jours.md

[^1_2]: https://www.geeksforgeeks.org/machine-learning/what-is-perceptron-the-simplest-artificial-neural-network/

[^1_3]: https://blog.damavis.com/en/simple-perceptron-mathematical-definition-and-properties/

[^1_4]: https://cs.stanford.edu/people/eroberts/courses/soco/projects/neural-networks/Neuron/index.html

[^1_5]: https://cs.stanford.edu/people/eroberts/courses/soco/projects/neural-networks/Neuron/

[^1_6]: https://www.picsellia.com/post/activation-functions-and-neural-networks

[^1_7]: https://www.tensorflow.org/guide/core/mlp_core

[^1_8]: https://www.tensorflow.org/tutorials/customization/custom_training_walkthrough

[^1_9]: https://playground.tensorflow.org

[^1_10]: https://deepwiki.com/tensorflow/playground/2.2-visualizations

[^1_11]: https://blog.tensorflow.org/2019/02/mit-introduction-to-deep-learning.html

[^1_12]: https://www.tensorflow.org/guide/keras/working_with_rnns

[^1_13]: https://www.tensorflow.org/tutorials/distribute/dtensor_ml_tutorial

[^1_14]: https://www.tensorflow.org/guide/autodiff

[^1_15]: https://www.tensorflow.org/tensorboard

[^1_16]: https://js.tensorflow.org/api/1.0.0/

[^1_17]: https://projector.tensorflow.org

[^1_18]: https://fr.wikipedia.org/wiki/Perceptron

[^1_19]: https://www.math.univ-toulouse.fr/~xgendre/ens/m2se/M2SE_TP5_en.pdf

[^1_20]: https://www.reddit.com/r/learnmachinelearning/comments/18can28/using_activation_function_before_taking_dot/

