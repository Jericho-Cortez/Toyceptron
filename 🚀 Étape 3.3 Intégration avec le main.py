<img src="https://r2cdn.perplexity.ai/pplx-full-logo-primary-dark%402x.png" style="height:64px;margin-right:32px"/>

# üöÄ √âtape 3.3 : Int√©gration avec le main.py - Guide Complet

## üéØ Objectif de cette √©tape

Faire fonctionner le `main.py` fourni par ton prof avec ton code (`neuron.py`, `layer.py`, `network.py`) sans erreur, et comprendre **chaque partie** de l'int√©gration.[^1]

***

## üìã Plan d'action

1. **Analyser le main.py** : comprendre ce qu'il attend
2. **Adapter ton code** : respecter l'API exacte
3. **Tester chaque composant** : Neuron ‚Üí Layer ‚Üí Network
4. **Valider les r√©sultats** : v√©rifier que les sorties sont correctes

***

## üîç PARTIE 1 : Analyse du main.py

### **Ce que fait le main.py**[^2]

Le fichier test **3 niveaux** d'abstraction :

```
1. Neuron individuel  ‚Üí Test du calcul de base
2. Layer (couche)     ‚Üí Test de plusieurs neurones
3. Network (r√©seau)   ‚Üí Test de plusieurs couches encha√Æn√©es
```


### **Imports n√©cessaires**

```python
from network import Network
from layer import Layer
from neuron import Neuron
from activations import act_relu, act_threshold, act_identity
```

‚ö†Ô∏è **Attention** : Le main.py importe depuis `activations` (avec un 's'), v√©rifie le nom de ton fichier !

***

## üß± PARTIE 2 : Impl√©mentation compl√®te

### **üìÑ 1. `neuron.py` - Le composant de base**

```python
class Neuron:
    """
    Un neurone effectue un produit scalaire + biais.
    
    Formule math√©matique :
        z = w‚ÇÅ¬∑x‚ÇÅ + w‚ÇÇ¬∑x‚ÇÇ + ... + w‚Çô¬∑x‚Çô + b
    
    Pas d'activation ici ! Le neurone retourne juste z (valeur brute).
    """
    
    def __init__(self, weights, bias=0.0):
        """
        Initialise un neurone avec ses poids et son biais.
        
        Args:
            weights (list): Liste des poids [w1, w2, ..., wn]
            bias (float): Biais du neurone (par d√©faut 0.0)
        
        Exemple:
            n = Neuron(weights=[0.2, -0.1, 0.4], bias=0.0)
        """
        self.weights = weights
        self.bias = bias
    
    def forward(self, inputs):
        """
        Calcule la sortie brute du neurone (forward pass).
        
        Args:
            inputs (list): Vecteur d'entr√©e [x1, x2, ..., xn]
        
        Returns:
            float: Valeur brute z (non activ√©e)
        
        Exemple:
            >>> n = Neuron([0.2, -0.1, 0.4], 0.0)
            >>> n.forward([1.0, 2.0, 4.0])
            1.6
            
        Explication du calcul :
            z = 0.2*1.0 + (-0.1)*2.0 + 0.4*4.0 + 0.0
            z = 0.2 - 0.2 + 1.6 + 0.0
            z = 1.6
        """
        # Produit scalaire manuel (sans numpy)
        z = 0.0
        for weight, input_value in zip(self.weights, inputs):
            z += weight * input_value
        
        # Ajout du biais
        z += self.bias
        
        return z


# ============================================
# TEST UNITAIRE (ne s'ex√©cute que si lanc√© directement)
# ============================================
if __name__ == "__main__":
    print("="*50)
    print("TEST NEURON")
    print("="*50)
    
    # Cr√©ation d'un neurone avec des poids fix√©s
    n1 = Neuron(weights=[0.2, -0.1, 0.4], bias=0.0)
    
    # Test avec l'entr√©e du main.py
    x = [1.0, 2.0, 4.0]
    result = n1.forward(x)
    
    print(f"Input: {x}")
    print(f"Poids: {n1.weights}")
    print(f"Biais: {n1.bias}")
    print(f"R√©sultat: {result}")
    print(f"Attendu: 1.6")
    print(f"‚úÖ Test OK" if abs(result - 1.6) < 0.0001 else "‚ùå Erreur")
```


#### **üí° Pourquoi ce design ?**

- **Pas d'activation dans le neurone** : Cela permet de r√©utiliser le m√™me neurone avec diff√©rentes activations[^2]
- **Retourne un scalaire** : Un neurone produit une seule valeur, pas une liste
- **Produit scalaire manuel** : On fait `z = Œ£(w¬∑x)` sans biblioth√®que externe

***

### **üìÑ 2. `layer.py` - Collection de neurones**

```python
from neuron import Neuron


class Layer:
    """
    Une couche = plusieurs neurones qui re√ßoivent les M√äMES inputs.
    
    Sch√©ma mental:
        Input: [x1, x2, x3]
               ‚Üì   ‚Üì   ‚Üì
           Neuron1  Neuron2  Neuron3
               ‚Üì       ‚Üì       ‚Üì
           [out1,   out2,   out3]
    
    Sur TensorFlow Playground, une colonne de cercles = 1 Layer.
    """
    
    def __init__(self, weights_list, biases_list):
        """
        Initialise une couche avec plusieurs neurones.
        
        Args:
            weights_list (list of lists): 
                Chaque sous-liste = poids d'un neurone
                Exemple: [[0.2, -0.1, 0.4], [-0.4, 0.3, 0.1]]
                         ‚îî‚îÄ neurone 1 ‚îÄ‚îò   ‚îî‚îÄ neurone 2 ‚îÄ‚îò
            
            biases_list (list): 
                Liste des biais, un par neurone
                Exemple: [0.0, 0.1]
        
        Contrainte importante:
            len(weights_list) == len(biases_list)
            (autant de lignes de poids que de biais)
        """
        self.neurons = []
        
        # Cr√©er un neurone pour chaque ligne de poids
        for weights, bias in zip(weights_list, biases_list):
            neuron = Neuron(weights=weights, bias=bias)
            self.neurons.append(neuron)
    
    def forward(self, inputs):
        """
        Propage les inputs √† travers tous les neurones de la couche.
        
        Args:
            inputs (list): Vecteur d'entr√©e [x1, x2, ...]
        
        Returns:
            list: Sorties brutes de chaque neurone [z1, z2, ...]
        
        Exemple:
            >>> layer = Layer(
            ...     weights_list=[[0.2, -0.1, 0.4], [-0.4, 0.3, 0.1]],
            ...     biases_list=[0.0, 0.1]
            ... )
            >>> layer.forward([1.0, 2.0, 4.0])
            [1.6, 0.7]
        
        Explication:
            - Neurone 1: 0.2*1.0 + (-0.1)*2.0 + 0.4*4.0 + 0.0 = 1.6
            - Neurone 2: (-0.4)*1.0 + 0.3*2.0 + 0.1*4.0 + 0.1 = 0.7
        """
        outputs = []
        
        # Chaque neurone calcule sa sortie ind√©pendamment
        for neuron in self.neurons:
            output = neuron.forward(inputs)
            outputs.append(output)
        
        return outputs


# ============================================
# TEST UNITAIRE
# ============================================
if __name__ == "__main__":
    print("="*50)
    print("TEST LAYER")
    print("="*50)
    
    # Cr√©ation d'une couche avec 2 neurones
    layer = Layer(
        weights_list=[
            [0.2, -0.1, 0.4],   # Poids neurone 1
            [-0.4, 0.3, 0.1],   # Poids neurone 2
        ],
        biases_list=[0.0, 0.1]
    )
    
    # Test
    x = [1.0, 2.0, 4.0]
    result = layer.forward(x)
    
    print(f"Input: {x}")
    print(f"Nombre de neurones: {len(layer.neurons)}")
    print(f"R√©sultat: {result}")
    print(f"Attendu: [1.6, 0.7]")
    print(f"‚úÖ Test OK" if result == [1.6, 0.7] else "‚ùå Erreur")
```


#### **üí° Points cl√©s**

- **Tous les neurones re√ßoivent les m√™mes inputs** : C'est le principe d'une couche fully-connected
- **Retourne une liste** : Autant de valeurs que de neurones
- **Pas d'activation** : La couche retourne les valeurs brutes, l'activation se fait dans le Network

***

### **üìÑ 3. `network.py` - Orchestration compl√®te**

```python
from layer import Layer


class Network:
    """
    Un r√©seau de neurones = plusieurs couches encha√Æn√©es.
    
    Flux de donn√©es:
        Input ‚Üí Layer 1 ‚Üí Activation ‚Üí Layer 2 ‚Üí Activation ‚Üí ... ‚Üí Output
    
    L'activation est appliqu√©e APR√àS chaque couche, pas DANS la couche.
    """
    
    def __init__(self, input_size, activation):
        """
        Initialise un r√©seau vide.
        
        Args:
            input_size (int): Nombre d'entr√©es du r√©seau (ex: 3)
            activation (function): Fonction d'activation √† appliquer
                                   (ex: act_sigmoid, act_relu)
        
        Exemple:
            >>> from math import exp
            >>> def act_sigmoid(x):
            ...     return 1 / (1 + exp(-x))
            >>> net = Network(input_size=3, activation=act_sigmoid)
        """
        self.input_size = input_size
        self.activation = activation
        self.layers = []  # Liste vide, on ajoutera les couches avec add()
    
    def add(self, weights, biases):
        """
        Ajoute une couche au r√©seau.
        
        Args:
            weights (list of lists): Matrice de poids
                [[w11, w12, ...], [w21, w22, ...], ...]
            biases (list): Vecteur de biais [b1, b2, ...]
        
        Exemple:
            >>> net.add(
            ...     weights=[[0.2, -0.1, 0.4], [-0.4, 0.3, 0.1]],
            ...     biases=[0.0, 0.1]
            ... )
        
        Remarque:
            - La premi√®re couche doit avoir len(weights[^0]) == input_size
            - Les couches suivantes doivent avoir len(weights[^0]) == 
              nombre de neurones de la couche pr√©c√©dente
        """
        layer = Layer(weights_list=weights, biases_list=biases)
        self.layers.append(layer)
    
    def feedforward(self, inputs):
        """
        Propage les inputs √† travers tout le r√©seau (forward pass).
        
        √âtapes:
            1. Calcul brut : Layer.forward(inputs)
            2. Activation : appliquer self.activation √† chaque sortie
            3. Les sorties activ√©es deviennent les inputs de la couche suivante
            4. R√©p√©ter pour toutes les couches
        
        Args:
            inputs (list): Vecteur d'entr√©e [x1, x2, ...]
        
        Returns:
            list: Sorties finales activ√©es du r√©seau
        
        Exemple complet:
            >>> from math import exp
            >>> def act_sigmoid(x):
            ...     return 1 / (1 + exp(-x))
            >>> 
            >>> net = Network(input_size=3, activation=act_sigmoid)
            >>> net.add(weights=[[0.2, -0.1, 0.4], [-0.4, 0.3, 0.1]], 
            ...         biases=[0.0, 0.1])
            >>> net.feedforward([1.0, 2.0, 4.0])
            [0.8320183851339245, 0.6681877721681662]
        
        Explication math√©matique:
            Couche 1 brut: [1.6, 0.7]
            Couche 1 activ√©: [sigmoid(1.6), sigmoid(0.7)]
                           = [0.832..., 0.668...]
        """
        current = inputs  # Les inputs initiaux
        
        # Boucle sur chaque couche
        for layer in self.layers:
            # 1. Forward brut (sans activation)
            raw_outputs = layer.forward(current)
            
            # 2. Application de l'activation sur chaque sortie
            activated_outputs = [self.activation(z) for z in raw_outputs]
            
            # 3. Les sorties activ√©es deviennent les inputs de la couche suivante
            current = activated_outputs
        
        # Retourner la derni√®re sortie activ√©e
        return current


# ============================================
# TEST UNITAIRE COMPLET
# ============================================
if __name__ == "__main__":
    from math import exp
    
    def act_sigmoid(x):
        return 1 / (1 + exp(-x))
    
    print("="*50)
    print("TEST NETWORK")
    print("="*50)
    
    # Cr√©ation du r√©seau
    net = Network(input_size=3, activation=act_sigmoid)
    
    # Ajout de la premi√®re couche (3 inputs ‚Üí 2 neurones)
    net.add(
        weights=[
            [0.2, -0.1, 0.4],
            [-0.4, 0.3, 0.1],
        ],
        biases=[0.0, 0.1]
    )
    
    # Test
    x = [1.0, 2.0, 4.0]
    result = net.feedforward(x)
    
    print(f"Input: {x}")
    print(f"Nombre de couches: {len(net.layers)}")
    print(f"R√©sultat: {result}")
    print(f"Attendu: [0.832..., 0.668...]")
    print(f"‚úÖ Test OK" if 0.83 < result[^0] < 0.84 else "‚ùå Erreur")
```


#### **üí° Logique de feedforward**

```python
# Visualisation du flux
Input: [1.0, 2.0, 4.0]
    ‚Üì
Layer 1 forward ‚Üí [1.6, 0.7]                    (brut)
    ‚Üì
Activation      ‚Üí [0.832, 0.668]                 (activ√©)
    ‚Üì
Layer 2 forward ‚Üí [0.282, 0.117, 0.116]         (brut)
    ‚Üì
Activation      ‚Üí [0.570, 0.529, 0.529]         (activ√©)
    ‚Üì
Layer 3 forward ‚Üí [0.123, -0.020]               (brut)
    ‚Üì
Activation      ‚Üí [0.530, 0.494]                (activ√© - OUTPUT FINAL)
```


***

### **üìÑ 4. `activations.py` - Fonctions d'activation**

```python
def act_identity(x):
    """
    Fonction identit√© : f(x) = x
    
    Utilis√©e pour les couches de sortie en r√©gression.
    """
    return x


def act_threshold(x):
    """
    Fonction de seuil (Heaviside) : f(x) = 1 si x ‚â• 0, sinon 0
    
    Utilis√©e dans les perceptrons classiques (portes logiques AND, OR).
    
    Exemple:
        >>> act_threshold(0.5)
        1.0
        >>> act_threshold(-0.2)
        0.0
    """
    return 1.0 if x >= 0 else 0.0


def act_relu(x):
    """
    Fonction ReLU (Rectified Linear Unit) : f(x) = max(0, x)
    
    Tr√®s utilis√©e dans les r√©seaux modernes (introduit la non-lin√©arit√©).
    
    Exemple:
        >>> act_relu(2.5)
        2.5
        >>> act_relu(-1.3)
        0.0
    """
    return max(0.0, x)


# Sigmoid est d√©fini dans le main.py, pas besoin de le dupliquer ici
```


***

## üß™ PARTIE 3 : Tests et validation

### **Test 1 : Neurone individuel**

```python
# Dans le main.py
n1 = Neuron(weights=[0.2, -0.1, 0.4], bias=0.0)
out_n1 = n1.forward([1.0, 2.0, 4.0])
# R√©sultat attendu: 1.6
```

**Calcul manuel** :

```
z = 0.2√ó1.0 + (‚àí0.1)√ó2.0 + 0.4√ó4.0 + 0.0
z = 0.2 ‚àí 0.2 + 1.6 + 0.0
z = 1.6 ‚úÖ
```


***

### **Test 2 : Couche**

```python
layer = Layer(
    weights_list=[[0.2, -0.1, 0.4], [-0.4, 0.3, 0.1]],
    biases_list=[0.0, 0.1]
)
raw = layer.forward([1.0, 2.0, 4.0])
# R√©sultat attendu: [1.6, 0.7]
```

**Calcul manuel** :

```
Neurone 1: 0.2√ó1.0 ‚àí 0.1√ó2.0 + 0.4√ó4.0 + 0.0 = 1.6 ‚úÖ
Neurone 2: ‚àí0.4√ó1.0 + 0.3√ó2.0 + 0.1√ó4.0 + 0.1 = 0.7 ‚úÖ
```


***

### **Test 3 : R√©seau complet**

```python
net = Network(input_size=3, activation=act_sigmoid)
net.add(weights=[[0.2, -0.1, 0.4], [-0.4, 0.3, 0.1]], biases=[0.0, 0.1])
net.add(weights=[[0.5, -0.2], [-0.3, 0.4], [0.1, 0.2]], biases=[0.0, 0.1, -0.1])
net.add(weights=[[0.3, -0.1, 0.2], [-0.5, 0.4, 0.1]], biases=[-0.1, 0.0])

y = net.feedforward([1.0, 2.0, 4.0])
# R√©sultat attendu: [0.5309442148001715, 0.494901997674804]
```

**Tu as obtenu exactement √ßa** ‚úÖ

***

## üéØ PARTIE 4 : Checklist finale

| Crit√®re | Statut | D√©tail |
| :-- | :-- | :-- |
| **Neuron.forward()** | ‚úÖ | Retourne un scalaire brut |
| **Layer.forward()** | ‚úÖ | Retourne une liste de scalaires bruts |
| **Network.add()** | ‚úÖ | Ajoute dynamiquement des couches |
| **Network.feedforward()** | ‚úÖ | Applique activation apr√®s chaque couche |
| **Network.layers** | ‚úÖ | Attribut accessible (liste des couches) |
| **Compatibilit√© main.py** | ‚úÖ | Toutes les sorties correspondent |


***

## üèÜ R√©sultat final obtenu

```
Input: [1.0, 2.0, 4.0]

--- Test Neuron ---
Neurone h1 (brut): 1.6                         ‚úÖ
Neurone h2 (brut): 0.7                         ‚úÖ

--- Test Layer ---
Couche (valeurs brutes): [1.6, 0.7]            ‚úÖ

--- Test Network ---
Sorties activ√©es : [0.5309..., 0.4949...]      ‚úÖ
```

**üéâ TU AS R√âUSSI L'INT√âGRATION COMPL√àTE !**

***

## üí° Points cl√©s √† retenir

### **1. S√©paration des responsabilit√©s**

- `Neuron` et `Layer` : calcul brut uniquement
- `Network` : g√®re l'activation et l'encha√Ænement


### **2. Architecture modulaire**

```
Neuron (produit scalaire + biais)
    ‚Üì
Layer (collection de Neuron)
    ‚Üì
Network (encha√Ænement de Layer + activation)
```


### **3. Pas de numpy, juste du Python pur**

- Produit scalaire manuel avec `zip()`
- List comprehensions pour l'activation
- Boucles for pour l'encha√Ænement

***

## üöÄ Prochaine √©tape

Tu es maintenant pr√™t pour l'**√âtape 3.4 : V√©rifications et ajustements**  o√π tu vas :[^1]

- Nettoyer ton code
- Ajouter des tests suppl√©mentaires
- Impl√©menter des exemples classiques (AND, OR, XOR)

**Bravo, ton Toyceptron est op√©rationnel ! üî•**

<div align="center">‚ÅÇ</div>

[^1]: ROADMAP-TOYCEPTRON-Mode-Sprint-3-4-jours.md

[^2]: main.py

