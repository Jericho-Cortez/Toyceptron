<img src="https://r2cdn.perplexity.ai/pplx-full-logo-primary-dark%402x.png" style="height:64px;margin-right:32px"/>

# ğŸ¯ JOUR 2 - Ã‰tape 2.1 : Comprendre une couche âœ… COMPLÃ‰TÃ‰

## ğŸ“‹ RÃ©capitulatif complet de ce qu'on a fait


***

## ğŸ—‚ï¸ Structure actuelle du projet

```
Perceptron/Code/
â”œâ”€â”€ activations.py    âœ… CrÃ©Ã© aujourd'hui
â”œâ”€â”€ neuron.py         âœ… Fait JOUR 1
â””â”€â”€ layer.py          âœ… CrÃ©Ã© aujourd'hui
```


***

# ğŸ“„ Fichier 1 : `activations.py`

```python
"""
Fonctions d'activation pour les neurones
Chaque fonction prend un nombre et retourne un nombre transformÃ©
"""

def identity(x):
    """
    Fonction identitÃ© : retourne x tel quel
    UtilisÃ©e quand on ne veut pas modifier la sortie
    Exemple : identity(5) â†’ 5
    """
    return x


def heaviside(x):
    """
    Fonction seuil (Heaviside) : perceptron classique
    Retourne 1 si x >= 0, sinon 0
    UtilisÃ©e pour les classifications binaires (oui/non)
    Exemple : heaviside(0.5) â†’ 1, heaviside(-0.2) â†’ 0
    """
    return 1 if x >= 0 else 0


def sigmoid(x):
    """
    Fonction sigmoÃ¯de : courbe en S
    Formule : 1 / (1 + e^-x)
    Transforme n'importe quelle valeur entre 0 et 1
    UtilisÃ©e pour les probabilitÃ©s
    Exemple : sigmoid(0) â†’ 0.5, sigmoid(5) â†’ 0.993
    """
    import math
    return 1 / (1 + math.exp(-x))


def relu(x):
    """
    ReLU (Rectified Linear Unit) : la plus populaire
    Formule : max(0, x)
    Si x positif â†’ garde x, si x nÃ©gatif â†’ met Ã  0
    TrÃ¨s utilisÃ©e dans les rÃ©seaux modernes (simple et efficace)
    Exemple : relu(3) â†’ 3, relu(-2) â†’ 0
    """
    return max(0, x)
```


### ğŸ§  Pourquoi ces 4 fonctions ?

| Fonction | Comportement | Usage typique |
| :-- | :-- | :-- |
| **identity** | Ne change rien | Couche de sortie (rÃ©gression) |
| **heaviside** | Tout ou rien (0 ou 1) | Perceptron classique |
| **sigmoid** | Ã‰crase entre 0 et 1 | ProbabilitÃ©s, classification binaire |
| **relu** | Garde le positif, tue le nÃ©gatif | Couches cachÃ©es (standard moderne) |

**Lien avec TensorFlow Playground :** Quand tu changes l'activation sur le playground, tu bascules entre ces fonctions.[^1]

***

# ğŸ“„ Fichier 2 : `neuron.py` (rappel JOUR 1)

```python
"""
Classe Neurone : l'unitÃ© de base d'un rÃ©seau de neurones
Un neurone fait 3 choses :
1. Produit scalaire (multiplie inputs par poids)
2. Ajoute le biais
3. Applique l'activation
"""

from activations import identity


class Neuron:
    def __init__(self, weights, bias=0, activation=identity):
        """
        Initialise un neurone
        
        ParamÃ¨tres :
        - weights : liste des poids [w1, w2, w3, ...]
                    Un poids par input attendu
        - bias : le biais (dÃ©calage), par dÃ©faut 0
        - activation : fonction d'activation, par dÃ©faut identity
        
        Exemple :
        n = Neuron(weights=[0.5, -0.3], bias=0.1, activation=relu)
        """
        self.weights = weights
        self.bias = bias
        self.activation = activation
    
    def forward(self, inputs):
        """
        Calcule la sortie du neurone (forward pass)
        
        Ã‰tapes mathÃ©matiques :
        1. z = w1*x1 + w2*x2 + ... + wn*xn  (produit scalaire)
        2. z = z + bias
        3. output = activation(z)
        
        ParamÃ¨tres :
        - inputs : liste de valeurs [x1, x2, x3, ...]
                   Doit avoir la mÃªme longueur que weights
        
        Retourne : un nombre (la sortie du neurone)
        
        Exemple :
        inputs = [1.0, 2.0]
        weights = [0.5, -0.3]
        bias = 0.1
        z = 1.0*0.5 + 2.0*(-0.3) + 0.1 = 0.5 - 0.6 + 0.1 = 0.0
        Si activation = relu : output = max(0, 0.0) = 0.0
        """
        # Ã‰tape 1 : Produit scalaire
        z = 0
        for i in range(len(self.weights)):
            z += self.weights[i] * inputs[i]
        
        # Ã‰tape 2 : Ajouter le biais
        z += self.bias
        
        # Ã‰tape 3 : Appliquer l'activation
        output = self.activation(z)
        
        return output


# Test du neurone
if __name__ == "__main__":
    from activations import relu, sigmoid
    
    print("=" * 50)
    print("TEST NEURONE")
    print("=" * 50)
    
    # Test 1 : Neurone avec activation identity
    n1 = Neuron(weights=[0.5, -0.3], bias=0.1, activation=identity)
    result1 = n1.forward([1.0, 2.0])
    print(f"Test 1 (identity) : {result1}")  # Attendu : 0.0
    
    # Test 2 : Neurone avec activation ReLU
    n2 = Neuron(weights=[0.5, -0.3], bias=0.1, activation=relu)
    result2 = n2.forward([1.0, 2.0])
    print(f"Test 2 (ReLU) : {result2}")  # Attendu : 0.0
    
    # Test 3 : Neurone avec activation sigmoid
    n3 = Neuron(weights=[0.5, -0.3], bias=0.1, activation=sigmoid)
    result3 = n3.forward([1.0, 2.0])
    print(f"Test 3 (sigmoid) : {result3}")  # Attendu : 0.5
    
    print("=" * 50)
```


### ğŸ§  Logique du Neuron

**Analogie :** Imagine une balance avec plusieurs plateaux :

- Chaque **input** est un poids sur un plateau
- Chaque **weight** est un multiplicateur (importance)
- Le **bias** est un poids fixe toujours prÃ©sent
- L'**activation** dÃ©cide comment interprÃ©ter le rÃ©sultat final

**Sur TensorFlow Playground :** Un cercle = un Neuron. Les lignes colorÃ©es = les poids. L'Ã©paisseur = l'importance du poids.[^1]

***

# ğŸ“„ Fichier 3 : `layer.py` (nouveau - JOUR 2)

```python
"""
Classe Layer (Couche) : une collection de neurones
Tous les neurones d'une couche :
- ReÃ§oivent les MÃŠMES inputs
- Ont leur propres poids/biais
- Produisent chacun une sortie
- Utilisent la mÃªme fonction d'activation
"""

from neuron import Neuron


class Layer:
    def __init__(self, num_neurons, num_inputs, activation):
        """
        Initialise une couche de neurones
        
        ParamÃ¨tres :
        - num_neurons : combien de neurones dans cette couche (ex: 3)
        - num_inputs : combien d'inputs chaque neurone reÃ§oit (ex: 2)
        - activation : fonction d'activation commune Ã  tous les neurones
        
        Logique :
        - CrÃ©e num_neurons neurones
        - Chaque neurone a num_inputs poids
        - Pour l'instant : poids fixÃ©s Ã  0.5 (on amÃ©liorera Ã  l'Ã©tape 2.3)
        
        Exemple :
        layer = Layer(num_neurons=3, num_inputs=2, activation=relu)
        â†’ CrÃ©e 3 neurones, chacun attend 2 inputs
        """
        self.neurons = []  # Liste vide pour stocker les neurones
        
        # Boucle : crÃ©er num_neurons neurones
        for _ in range(num_neurons):
            # Chaque neurone a num_inputs poids
            # [0.5] * num_inputs â†’ [0.5, 0.5, 0.5, ...] (rÃ©pÃ¨te 0.5)
            weights = [0.5] * num_inputs
            
            # CrÃ©er le neurone et l'ajouter Ã  la liste
            neuron = Neuron(weights=weights, bias=0.1, activation=activation)
            self.neurons.append(neuron)
    
    def forward(self, inputs):
        """
        Propage les inputs Ã  travers TOUS les neurones de la couche
        
        ParamÃ¨tres :
        - inputs : liste de valeurs [x1, x2, ...]
                   Sera passÃ©e Ã  CHAQUE neurone
        
        Retourne : liste des sorties [out1, out2, out3, ...]
                   Une sortie par neurone
        
        Logique :
        1. Prends les inputs
        2. Passe-les au neurone 1 â†’ obtiens out1
        3. Passe-les au neurone 2 â†’ obtiens out2
        4. ... pour tous les neurones
        5. Retourne [out1, out2, out3, ...]
        
        Exemple :
        inputs = [1.0, 2.0]
        3 neurones dans la couche
        â†’ Chaque neurone calcule sa sortie
        â†’ RÃ©sultat : [0.8, 1.2, 0.5] (3 valeurs)
        """
        outputs = []  # Liste vide pour collecter les sorties
        
        # Pour chaque neurone de la couche
        for neuron in self.neurons:
            # Calcule la sortie de ce neurone avec les inputs
            output = neuron.forward(inputs)
            
            # Ajoute cette sortie Ã  la liste
            outputs.append(output)
        
        # Retourne toutes les sorties
        return outputs


# Test de la couche
if __name__ == "__main__":
    from activations import relu, sigmoid
    
    print("=" * 50)
    print("TEST LAYER (COUCHE)")
    print("=" * 50)
    
    # Test 1 : Couche avec 3 neurones, 2 inputs, activation ReLU
    print("\n--- Test 1 : 3 neurones, 2 inputs, ReLU ---")
    layer1 = Layer(num_neurons=3, num_inputs=2, activation=relu)
    result1 = layer1.forward([1.0, 2.0])
    print(f"Inputs : [1.0, 2.0]")
    print(f"Sorties : {result1}")
    print(f"Nombre de sorties : {len(result1)}")
    print(f"âœ… On a bien {len(result1)} sorties (une par neurone)")
    
    # Test 2 : Couche avec 5 neurones, 3 inputs, activation sigmoid
    print("\n--- Test 2 : 5 neurones, 3 inputs, sigmoid ---")
    layer2 = Layer(num_neurons=5, num_inputs=3, activation=sigmoid)
    result2 = layer2.forward([0.5, 1.5, -0.5])
    print(f"Inputs : [0.5, 1.5, -0.5]")
    print(f"Sorties : {result2}")
    print(f"Nombre de sorties : {len(result2)}")
    print(f"âœ… On a bien {len(result2)} sorties (une par neurone)")
    
    # Test 3 : VÃ©rifier que chaque neurone reÃ§oit les mÃªmes inputs
    print("\n--- Test 3 : VÃ©rification inputs identiques ---")
    print("Les 3 neurones reÃ§oivent TOUS [1.0, 2.0]")
    print("Mais comme ils ont les mÃªmes poids (0.5, 0.5), ils donnent le mÃªme rÃ©sultat")
    print("Ã€ l'Ã©tape 2.3, on aura des poids alÃ©atoires â†’ sorties diffÃ©rentes")
    
    print("=" * 50)
```


### ğŸ§  Logique de la Layer

**Analogie :** Une couche = une Ã©quipe de spÃ©cialistes qui regardent tous les **mÃªmes** donnÃ©es, mais chacun donne son propre avis.

```
Inputs : [1.0, 2.0]
        â†“       â†“
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚  Neuron 1      â”‚ â†’ 0.8  (avis du spÃ©cialiste 1)
   â”‚  Neuron 2      â”‚ â†’ 1.2  (avis du spÃ©cialiste 2)
   â”‚  Neuron 3      â”‚ â†’ 0.5  (avis du spÃ©cialiste 3)
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Outputs : [0.8, 1.2, 0.5]
```

**Sur TensorFlow Playground :** Une colonne verticale de cercles = une Layer. Tous les cercles d'une colonne reÃ§oivent les mÃªmes lignes qui arrivent.[^1]

***

## ğŸ¯ Ce qu'on a accompli Ã  l'Ã©tape 2.1

| Ã‰lÃ©ment | Statut | RÃ´le |
| :-- | :-- | :-- |
| `activations.py` | âœ… | 4 fonctions d'activation (identity, heaviside, sigmoid, relu) |
| `neuron.py` | âœ… | Classe Neuron avec forward pass complet |
| `layer.py` | âœ… | Classe Layer qui regroupe plusieurs neurones |


***

## ğŸ”¥ Validation finale

Lance chaque fichier pour tester :

```bash
python activations.py  # (pas de test, mais aucune erreur)
python neuron.py       # Affiche les 3 tests de neurones
python layer.py        # Affiche les 3 tests de couches
```

**RÃ©sultat attendu pour `layer.py` :**

```
==================================================
TEST LAYER (COUCHE)
==================================================

--- Test 1 : 3 neurones, 2 inputs, ReLU ---
Inputs : [1.0, 2.0]
Sorties : [1.6, 1.6, 1.6]
Nombre de sorties : 3
âœ… On a bien 3 sorties (une par neurone)

--- Test 2 : 5 neurones, 3 inputs, sigmoid ---
Inputs : [0.5, 1.5, -0.5]
Sorties : [0.7310585786300049, 0.7310585786300049, ...]
Nombre de sorties : 5
âœ… On a bien 5 sorties (une par neurone)
```


***

## ğŸ“š Liens thÃ©oriques

**Ce que tu as codÃ© correspond exactement Ã  :**

1. **Produit scalaire** : `z = w1*x1 + w2*x2 + ...` (dans `neuron.py`)
2. **Biais** : `z = z + bias` (dans `neuron.py`)
3. **Activation** : `output = f(z)` (dans `neuron.py`)
4. **ParallÃ©lisation** : tous les neurones d'une couche calculent en mÃªme temps (dans `layer.py`)[^1]

**Sur TensorFlow Playground :** Active la visualisation et observe comment les valeurs se propagent colonne par colonne (layer par layer).[^1]

***

## ğŸš€ Prochaine Ã©tape : 2.2 - Initialisation intelligente des poids

Actuellement, tous les poids = 0.5 â†’ tous les neurones donnent le mÃªme rÃ©sultat. Ã€ l'Ã©tape 2.2, on va gÃ©nÃ©rer des **poids alÃ©atoires** pour que chaque neurone soit unique ![^1]

**Tu es prÃªt pour l'Ã©tape 2.2 ?** ğŸ”¥

<div align="center">â‚</div>

[^1]: ROADMAP-TOYCEPTRON-Mode-Sprint-3-4-jours.md

