# Sprint Toyceptron - JOUR 2 - √âtape 2.2 : Coder layer.py

## Concept th√©orique : Qu'est-ce qu'une Layer ?

### D√©finition simple

Une **Layer (couche)** = **collection de neurones** qui travaillent **en parall√®le**.

**R√®gle d'or :**

- Tous les neurones d'une couche re√ßoivent **les m√™mes inputs**
- Mais chaque neurone a **ses propres poids** (g√©n√©r√©s al√©atoirement)
- Donc chaque neurone produit **une sortie diff√©rente**


### Sch√©ma visuel

```
Inputs: [x1, x2, x3]
     ‚Üì      ‚Üì     ‚Üì
   [Neuron1] [Neuron2] [Neuron3]  ‚Üê Layer (3 neurones)
   w=[...1]  w=[...2]  w=[...3]   ‚Üê Poids diff√©rents
     ‚Üì         ‚Üì         ‚Üì
  [out1,     out2,     out3]       ‚Üê 3 sorties (liste)
```


### Lien avec TensorFlow Playground

Sur [playground.tensorflow.org](https://playground.tensorflow.org)  :

- **1 colonne de cercles** = 1 Layer
- **Chaque cercle** = 1 Neuron
- **Lignes qui arrivent** = inputs (identiques pour toute la colonne)
- **Couleur/√©paisseur des lignes** = valeur des poids (al√©atoires au d√©part)

***

## Code complet comment√© de `layer.py`

```python
# layer.py
import random
from neuron import Neuron


class Layer:
    """
    Couche de neurones (fully-connected).
    Tous les neurones de la couche re√ßoivent les m√™mes inputs.
    
    Sch√©ma conceptuel:
        Inputs [x1, x2, ..., xn]
            ‚Üì ‚Üì ‚Üì (m√™mes inputs pour tous)
        [Neuron1, Neuron2, ..., NeuronM]
            ‚Üì ‚Üì ‚Üì
        Outputs [y1, y2, ..., yM]
    """
    
    def __init__(self, num_neurons, num_inputs, activation):
        """
        Initialise une couche avec plusieurs neurones.
        
        Args:
            num_neurons (int): Nombre de neurones dans la couche
            num_inputs (int): Nombre d'entr√©es pour chaque neurone
            activation (function): Fonction d'activation commune
        
        Exemple:
            Layer(num_neurons=3, num_inputs=2, activation=relu)
            ‚Üí Cr√©e 3 neurones
            ‚Üí Chacun a 2 poids (g√©n√©r√©s al√©atoirement)
            ‚Üí Tous utilisent la fonction relu
        """
        self.neurons = []  # Liste vide pour stocker les neurones
        
        # Boucle pour cr√©er num_neurons neurones
        for _ in range(num_neurons):
            # G√©n√©rer les poids al√©atoires ICI (Python pur, pas numpy!)
            weights = [random.uniform(-1, 1) for _ in range(num_inputs)]
            
            # Cr√©er le neurone avec les poids g√©n√©r√©s
            neuron = Neuron(
                weights=weights,        # Poids g√©n√©r√©s dans Layer
                bias=0,                 # Biais initialis√© √† 0
                activation=activation   # Fonction d'activation commune
            )
            self.neurons.append(neuron)  # Ajouter le neurone √† la liste
    
    def forward(self, inputs):
        """
        Passe avant (forward pass) de la couche.
        
        Processus:
        1. Chaque neurone de la couche re√ßoit les M√äMES inputs
        2. Chaque neurone calcule sa propre sortie (avec ses propres poids)
        3. On collecte toutes les sorties dans une liste
        
        Args:
            inputs (list): Liste des valeurs d'entr√©e
                          Taille: len(inputs) doit √©galer num_inputs
        
        Returns:
            list: Liste des sorties de chaque neurone
                  Taille: len(outputs) = num_neurons
        
        Exemple concret:
            inputs = [1.0, 2.0]  # 2 valeurs
            layer avec 3 neurones
            ‚Üí outputs = [0.5, -0.3, 0.8]  # 3 sorties (1 par neurone)
        """
        outputs = []  # Liste vide pour stocker les sorties
        
        # Boucle sur chaque neurone de la couche
        for neuron in self.neurons:
            # Chaque neurone traite les m√™mes inputs
            output = neuron.forward(inputs)  # Retourne 1 scalaire
            outputs.append(output)           # Ajouter √† la liste
        
        return outputs  # Retourner la liste compl√®te des sorties


# ========================================
# TESTS UNITAIRES
# ========================================
if __name__ == "__main__":
    import sys
    sys.path.append('.')  # Pour importer activations.py
    
    from activations import identity, relu, sigmoid
    
    print("="*50)
    print("TEST 1: Layer avec activation identity")
    print("="*50)
    
    # Cr√©er une couche: 3 neurones, 2 inputs chacun, activation identity
    layer = Layer(num_neurons=3, num_inputs=2, activation=identity)
    
    # Afficher les poids g√©n√©r√©s automatiquement (doivent √™tre al√©atoires)
    print(f"Neurone 1 poids: {layer.neurons[^0].weights}")
    print(f"Neurone 2 poids: {layer.neurons[^1].weights}")
    print(f"Neurone 3 poids: {layer.neurons[^2].weights}")
    
    # Forward pass avec [1.0, 2.0]
    result = layer.forward([1.0, 2.0])
    print(f"\nSorties de la couche: {result}")
    print(f"Type: {type(result)} | Longueur: {len(result)}")
    
    print("\n" + "="*50)
    print("TEST 2: Layer avec activation ReLU")
    print("="*50)
    
    # Couche avec 2 neurones, 3 inputs, activation relu
    layer_relu = Layer(num_neurons=2, num_inputs=3, activation=relu)
    result_relu = layer_relu.forward([1.0, -2.0, 3.0])
    print(f"Sorties avec ReLU: {result_relu}")
    
    print("\n" + "="*50)
    print("TEST 3: Layer avec activation sigmoid")
    print("="*50)
    
    # Couche avec 4 neurones, 2 inputs, activation sigmoid
    layer_sigmoid = Layer(num_neurons=4, num_inputs=2, activation=sigmoid)
    result_sigmoid = layer_sigmoid.forward([0.5, -0.5])
    print(f"Sorties avec sigmoid: {result_sigmoid}")
    
    print("\n‚úÖ Tous les tests layer.py sont OK !")
```


***

##  Explications d√©taill√©es ligne par ligne

### **PARTIE 1 : Imports**

```python
import random
from neuron import Neuron
```

**Pourquoi `random` ?**

- Module **natif Python** (pas numpy !)
- G√©n√®re des nombres al√©atoires pour initialiser les poids
- `random.uniform(-1, 1)` ‚Üí nombre al√©atoire entre -1 et 1

**Pourquoi `from neuron import Neuron` ?**

- On r√©utilise la classe `Neuron` cr√©√©e au JOUR 1
- Architecture modulaire : Layer **compose** des Neuron

***

### **PARTIE 2 : `__init__` - Construction de la couche**

#### Ligne par ligne :

```python
self.neurons = []
```

- Cr√©e une **liste vide** pour stocker tous les neurones de la couche
- Exemple : `[Neuron1, Neuron2, Neuron3]`

***

```python
for _ in range(num_neurons):
```

- Boucle qui tourne `num_neurons` fois
- `_` (underscore) = "je n'utilise pas la variable de boucle"
- Exemple : Si `num_neurons=3`, la boucle tourne 3 fois

***

```python
weights = [random.uniform(-1, 1) for _ in range(num_inputs)]
```

**D√©cortiquage complet :**


| Partie | Explication |
| :-- | :-- |
| `random.uniform(-1, 1)` | G√©n√®re 1 nombre al√©atoire entre -1 et 1 |
| `for _ in range(num_inputs)` | R√©p√®te `num_inputs` fois |
| `[...]` | List comprehension = cr√©e une liste |

**Exemple concret :**

```python
num_inputs = 3
weights = [random.uniform(-1, 1) for _ in range(3)]
# R√©sultat possible : [0.543, -0.821, 0.234]
```

**Pourquoi entre -1 et 1 ?**

- Bonne pratique en deep learning
- √âvite les valeurs trop grandes (explosion de gradient)
- √âvite les valeurs trop petites (vanishing gradient)

***

```python
neuron = Neuron(
    weights=weights,        # Poids g√©n√©r√©s juste avant
    bias=0,                 # Biais = 0 (simple pour commencer)
    activation=activation   # Fonction pass√©e en param√®tre
)
```

**Points importants :**

- **`weights=weights`** : On passe les poids g√©n√©r√©s (pas `None`)
- **Tous les neurones ont le m√™me `num_inputs`** (coh√©rence dimensionnelle)
- **Tous les neurones ont la m√™me `activation`** (simplifie l'architecture)
- **Mais chaque neurone a des poids DIFF√âRENTS** (g√©n√©r√©s al√©atoirement)

***

```python
self.neurons.append(neuron)
```

- Ajoute le neurone cr√©√© √† la liste `self.neurons`
- Apr√®s 3 it√©rations : `self.neurons = [Neuron1, Neuron2, Neuron3]`

***

### **PARTIE 3 : `forward` - Propagation avant**

#### Ligne par ligne :

```python
outputs = []
```

- Liste vide pour collecter les r√©sultats
- Contiendra 1 valeur par neurone

***

```python
for neuron in self.neurons:
```

- Parcourt **chaque neurone** de la couche
- Exemple : `neuron` = `Neuron1`, puis `Neuron2`, puis `Neuron3`

***

```python
output = neuron.forward(inputs)
```

**Point crucial** : **Tous les neurones re√ßoivent les m√™mes `inputs` !**

**Exemple d√©taill√© :**

```python
inputs = [1.0, 2.0]

# Neuron 1 (poids = [0.5, -0.3])
output1 = neuron1.forward([1.0, 2.0])
# ‚Üí 0.5√ó1.0 + (-0.3)√ó2.0 + 0 = -0.1

# Neuron 2 (poids = [0.8, 0.2])
output2 = neuron2.forward([1.0, 2.0])
# ‚Üí 0.8√ó1.0 + 0.2√ó2.0 + 0 = 1.2

# Neuron 3 (poids = [-0.4, 0.9])
output3 = neuron3.forward([1.0, 2.0])
# ‚Üí (-0.4)√ó1.0 + 0.9√ó2.0 + 0 = 1.4
```

**M√™me inputs, sorties diff√©rentes ‚Üí magie des poids diff√©rents !**

***

```python
outputs.append(output)
```

- Ajoute la sortie du neurone (1 scalaire) √† la liste
- Apr√®s 3 neurones : `outputs = [-0.1, 1.2, 1.4]`

***

```python
return outputs
```

- Retourne la **liste compl√®te** des sorties
- **Neuron retourne 1 scalaire, Layer retourne 1 liste !**

***

##  Exemple concret avec calculs complets

### Configuration

```python
layer = Layer(num_neurons=3, num_inputs=2, activation=identity)
```

**Ce qui se passe :**

1. Cr√©e une liste vide `neurons = []`
2. **It√©ration 1 :**
    - G√©n√®re `weights = [0.5, -0.3]` (al√©atoire)
    - Cr√©e `Neuron1(weights=[0.5, -0.3], bias=0, activation=identity)`
    - Ajoute √† la liste
3. **It√©ration 2 :**
    - G√©n√®re `weights = [0.8, 0.2]`
    - Cr√©e `Neuron2(weights=[0.8, 0.2], bias=0, activation=identity)`
4. **It√©ration 3 :**
    - G√©n√®re `weights = [-0.4, 0.9]`
    - Cr√©e `Neuron3(weights=[-0.4, 0.9], bias=0, activation=identity)`

**R√©sultat : `layer.neurons = [Neuron1, Neuron2, Neuron3]`**

***

### Forward pass

```python
result = layer.forward([1.0, 2.0])
```

**D√©roulement d√©taill√© :**


| √âtape | Neurone | Calcul | R√©sultat |
| :-- | :-- | :-- | :-- |
| 1 | Neuron1 | `0.5√ó1.0 + (-0.3)√ó2.0 + 0 = -0.1` | `-0.1` |
| 2 | Neuron2 | `0.8√ó1.0 + 0.2√ó2.0 + 0 = 1.2` | `1.2` |
| 3 | Neuron3 | `(-0.4)√ó1.0 + 0.9√ó2.0 + 0 = 1.4` | `1.4` |

**R√©sultat final : `[-0.1, 1.2, 1.4]`**

***

##  Analyse des tests unitaires

### **TEST 1 : Identity (pas de modification)**

```python
layer = Layer(num_neurons=3, num_inputs=2, activation=identity)
result = layer.forward([1.0, 2.0])
```

**R√©sultat attendu :**

```
Neurone 1 poids: [-0.079, -0.640]
Neurone 2 poids: [0.728, 0.481]
Neurone 3 poids: [-0.733, -0.897]

Sorties: [-1.358, 1.691, -2.527]
Type: <class 'list'> | Longueur: 3
```

** V√©rifications :**

- 3 neurones cr√©√©s
- Poids al√©atoires diff√©rents
- 3 sorties (type `list`)
- Identity ne modifie pas les valeurs

***

### **TEST 2 : ReLU (coupe les n√©gatifs)**

```python
layer_relu = Layer(num_neurons=2, num_inputs=3, activation=relu)
result_relu = layer_relu.forward([1.0, -2.0, 3.0])
```

**Comportement ReLU :**

```
Neuron 1 : z = ... ‚Üí 2.351 ‚Üí relu(2.351) = 2.351 
Neuron 2 : z = ... ‚Üí -0.456 ‚Üí relu(-0.456) = 0.0 
```

**ReLU = `max(0, x)`** ‚Üí garde les positifs, met les n√©gatifs √† 0

***

### **TEST 3 : Sigmoid (compresse entre 0 et 1)**

```python
layer_sigmoid = Layer(num_neurons=4, num_inputs=2, activation=sigmoid)
result_sigmoid = layer_sigmoid.forward([0.5, -0.5])
```

**Comportement sigmoid :**

```
Toutes les sorties entre 0 et 1 : [0.444, 0.421, 0.551, 0.635]
```

**Sigmoid = $\frac{1}{1 + e^{-x}}$** ‚Üí sortie toujours entre 0 et 1

***

##  Points cl√©s √† retenir

###  **Architecture**

```
Layer = collection de Neuron
- M√™me num_inputs pour tous
- M√™me activation pour tous
- Poids diff√©rents pour chacun
```


###  **G√©n√©ration de poids (Python pur)**

```python
weights = [random.uniform(-1, 1) for _ in range(num_inputs)]
```

-  Pas de numpy
-  Valeurs entre -1 et 1
-  Diff√©rentes √† chaque ex√©cution


###  **Forward pass**

```
M√™me inputs ‚Üí Tous les neurones
Poids diff√©rents ‚Üí Sorties diff√©rentes
1 neurone ‚Üí 1 scalaire
N neurones ‚Üí liste de N scalaires
```


###  **S√©paration des responsabilit√©s**

- **Neuron** : calcule produit scalaire + biais + activation
- **Layer** : g√®re la collection, g√©n√®re les poids, orchestre le forward

***

##  Checklist √âtape 2.2

- [x] Fichier `layer.py` cr√©√©
- [x] Classe `Layer` avec `__init__` et `forward`
- [x] Import `random` (Python pur)
- [x] G√©n√©ration poids al√©atoires avec list comprehension
- [x] Cr√©ation de `num_neurons` neurones avec boucle `for`
- [x] `forward()` retourne une liste (pas un scalaire)
- [x] Tests identity, relu, sigmoid
- [x] Poids diff√©rents √† chaque ex√©cution

***

##  Validation finale

**Lance :**

```bash
python layer.py
```

**R√©sultat attendu :**

```
 Tous les tests layer.py sont OK !
```

**Lance une 2√®me fois :**

```bash
python layer.py
```

**‚Üí Les poids doivent √™tre diff√©rents !** (preuve de la g√©n√©ration al√©atoire)

***

## üéì Ce que tu as appris

| Concept                | Maintenant                        |
| :--------------------- | :-------------------------------- |
| Layer                  | Collection de neurones parall√®les |
| Poids al√©atoires       | `random.uniform(-1, 1)`           |
| Forward d'une couche   | M√™me inputs, sorties diff√©rentes  |
| Architecture modulaire |  Layer compose des Neuron         |


