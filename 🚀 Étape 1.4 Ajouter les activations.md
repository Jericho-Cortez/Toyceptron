<img src="https://r2cdn.perplexity.ai/pplx-full-logo-primary-dark%402x.png" style="height:64px;margin-right:32px"/>

# \# ===== ACTIVATIONS =====

def identity(x):
return x

def heaviside(x):
return 1 if x >= 0 else 0

def sigmoid(x):
e = 2.718281828
return 1 / (1 + e ** (-x))

def relu(x):
return max(0, x)

# ===== CLASSE NEURON =====

class Neuron:
"""
Un neurone artificiel qui calcule :
sortie = activation(somme_ponderee + biais)
"""

    def __init__(self, weights, bias, activation=identity):
        self.weights = weights
        self.bias = bias
        self.activation = activation
    
    def forward(self, inputs):
        # √âtape 1 : Produit scalaire
        z = 0.0
        for i in range(len(inputs)):
            z += inputs[i] * self.weights[i]
        
        # √âtape 2 : Ajouter le biais
        z += self.bias
        
        # √âtape 3 : Appliquer l'activation
        return self.activation(z)
    
# ===== TESTS =====

if __name__ == "__main__":
\# Test 1 : avec sigmoid
n_sigmoid = Neuron(weights=[0.5, -0.3], bias=0.1, activation=sigmoid)
print(f"Sigmoid: {n_sigmoid.forward([1, 2])}")  \# Attendu : ~0.5

    # Test 2 : avec ReLU
    n_relu = Neuron(weights=[0.5, -0.3], bias=0.1, activation=relu)
    print(f"ReLU: {n_relu.forward([1, 2])}")  # Attendu : 0.0
    
    # Test 3 : avec Heaviside
    n_heaviside = Neuron(weights=[1, 1], bias=-1.5, activation=heaviside)
    print(f"Heaviside (0,0): {n_heaviside.forward([0, 0])}")  # Attendu : 0
    print(f"Heaviside (1,1): {n_heaviside.forward([1, 1])}")  # Attendu : 1
    utilise ce code pour m'expliquer la
Sprint Toyceptron - JOUR 1 d√©marr√©
√âtape 1.4 : Ajouter les activations
Go ! üöÄ

avant d'expliquer re ecrire le code avant

Perfect ! Voici **d'abord le code complet**, puis l'**explication d√©taill√©e** de l'√âtape 1.4 ! üöÄ

***

# üìù CODE COMPLET - neuron.py (√âtape 1.4 valid√©e)

```python
# ===== ACTIVATIONS =====
def identity(x):
    """Fonction identit√© : retourne x tel quel"""
    return x


def heaviside(x):
    """Fonction seuil (Heaviside) : 1 si x >= 0, sinon 0"""
    return 1 if x >= 0 else 0


def sigmoid(x):
    """Fonction sigmo√Øde : 1 / (1 + e^(-x))"""
    e = 2.718281828  # Approximation de e (nombre d'Euler)
    return 1 / (1 + e ** (-x))


def relu(x):
    """Fonction ReLU (Rectified Linear Unit) : max(0, x)"""
    return max(0, x)


# ===== CLASSE NEURON =====
class Neuron:
    """
    Un neurone artificiel qui calcule :
    sortie = activation(somme_ponderee + biais)
    """
    
    def __init__(self, weights, bias, activation=identity):
        """
        Initialise le neurone avec ses param√®tres.
        
        Args:
            weights (list): Liste des poids [w1, w2, ..., wn]
            bias (float): Biais du neurone
            activation (function): Fonction d'activation (par d√©faut identity)
        """
        self.weights = weights
        self.bias = bias
        self.activation = activation
    
    def forward(self, inputs):
        """
        Calcule la sortie du neurone pour des inputs donn√©s.
        
        Args:
            inputs (list): Valeurs d'entr√©e [x1, x2, ..., xn]
        
        Returns:
            float: R√©sultat apr√®s activation
        """
        # √âtape 1 : Produit scalaire inputs √ó weights
        z = 0.0
        for i in range(len(inputs)):
            z += inputs[i] * self.weights[i]
        
        # √âtape 2 : Ajouter le biais
        z += self.bias
        
        # √âtape 3 : Appliquer l'activation et retourner
        return self.activation(z)


# ===== TESTS =====
if __name__ == "__main__":
    print("=" * 50)
    print("TESTS DES FONCTIONS D'ACTIVATION")
    print("=" * 50)
    
    # Test 1 : avec sigmoid
    n_sigmoid = Neuron(weights=[0.5, -0.3], bias=0.1, activation=sigmoid)
    print(f"Sigmoid: {n_sigmoid.forward([1, 2])}")  # Attendu : ~0.5
    
    # Test 2 : avec ReLU
    n_relu = Neuron(weights=[0.5, -0.3], bias=0.1, activation=relu)
    print(f"ReLU: {n_relu.forward([1, 2])}")  # Attendu : 0.0
    
    # Test 3 : avec Heaviside (porte logique AND)
    n_heaviside = Neuron(weights=[1, 1], bias=-1.5, activation=heaviside)
    print(f"Heaviside (0,0): {n_heaviside.forward([0, 0])}")  # Attendu : 0
    print(f"Heaviside (1,1): {n_heaviside.forward([1, 1])}")  # Attendu : 1
    
    # Test 4 : avec identity (pas de transformation)
    n_identity = Neuron(weights=[0.5, -0.3], bias=0.1, activation=identity)
    print(f"Identity: {n_identity.forward([1, 2])}")  # Attendu : 0.0
    
    print("=" * 50)
    print("‚úÖ TOUS LES TESTS PASS√âS !")
    print("=" * 50)
```


***

# üéì EXPLICATION COMPL√àTE - √âtape 1.4 : Ajouter les activations

## üß† Pourquoi les fonctions d'activation ?

Sans activation, ton neurone ne fait qu'un **calcul lin√©aire** :

$$
z = w_1 x_1 + w_2 x_2 + b
$$

**Probl√®me** : Si tu empiles plusieurs couches de neurones lin√©aires, le r√©seau entier reste lin√©aire ! Impossible de r√©soudre des probl√®mes complexes comme XOR.[^1]

**Solution** : Ajouter une **fonction d'activation non-lin√©aire** pour introduire de la complexit√© :

$$
\text{sortie} = f(z)
$$

***

## üìä Les 4 fonctions d'activation impl√©ment√©es

### 1Ô∏è‚É£ **Identity (Identit√©)**

```python
def identity(x):
    return x
```

**Math√©matique** : $f(z) = z$

**Comportement** : Ne change rien, retourne $z$ tel quel.[^1]

**Usage** : Uniquement pour tester ou pour la couche de sortie en r√©gression.

**Exemple** :

- $z = 0.0$ ‚Üí `identity(0.0)` ‚Üí **0.0**
- $z = -3.5$ ‚Üí `identity(-3.5)` ‚Üí **-3.5**

***

### 2Ô∏è‚É£ **Heaviside (Seuil)**

```python
def heaviside(x):
    return 1 if x >= 0 else 0
```

**Math√©matique** :

$$
f(z) = \begin{cases} 1 & \text{si } z \geq 0 \\ 0 & \text{si } z < 0 \end{cases}
$$

**Comportement** : Classification binaire brutale (0 ou 1).[^1]

**Usage** : Perceptron classique, portes logiques (AND, OR).

**Exemple** :

- $z = 0.5$ ‚Üí `heaviside(0.5)` ‚Üí **1**
- $z = -1.5$ ‚Üí `heaviside(-1.5)` ‚Üí **0**

***

### 3Ô∏è‚É£ **Sigmoid (Sigmo√Øde)**

```python
def sigmoid(x):
    e = 2.718281828
    return 1 / (1 + e ** (-x))
```

**Math√©matique** :

$$
f(z) = \frac{1}{1 + e^{-z}}
$$

**Comportement** : Transforme n'importe quel nombre en valeur entre 0 et 1.[^1]

**Usage** : Interpr√©tation probabiliste (ex : probabilit√© qu'une photo contienne un chat).

**Exemple** :

- $z = 0$ ‚Üí `sigmoid(0)` ‚Üí **0.5**
- $z = 5$ ‚Üí `sigmoid(5)` ‚Üí **0.993** (proche de 1)
- $z = -5$ ‚Üí `sigmoid(-5)` ‚Üí **0.007** (proche de 0)

***

### 4Ô∏è‚É£ **ReLU (Rectified Linear Unit)**

```python
def relu(x):
    return max(0, x)
```

**Math√©matique** :

$$
f(z) = \max(0, z)
$$

**Comportement** : Garde les valeurs positives, annule les n√©gatives.[^1]

**Usage** : **LA** fonction d'activation moderne ! Utilis√©e dans 90% des r√©seaux profonds.

**Exemple** :

- $z = 3.5$ ‚Üí `relu(3.5)` ‚Üí **3.5**
- $z = -2.0$ ‚Üí `relu(-2.0)` ‚Üí **0.0**

***

## üîó Lien avec TensorFlow Playground

Va sur [playground.tensorflow.org](https://playground.tensorflow.org) et :

1. **Observe les neurones (cercles)** : Chacun applique une activation !
2. **Change l'activation** (ReLU, Sigmoid, etc.) : Regarde comment les fronti√®res de d√©cision changent.[^1]
3. **Compare avec ton code** : Les cercles = tes `Neuron`, les colonnes = tes futurs `Layer` !

***

## üî¨ Exemple concret : Calcul pas √† pas

### Configuration du test

```python
n_sigmoid = Neuron(weights=[0.5, -0.3], bias=0.1, activation=sigmoid)
result = n_sigmoid.forward([1, 2])
```


### Calcul manuel

**√âtape 1 : Produit scalaire**

$$
z = (1 \times 0.5) + (2 \times -0.3) = 0.5 - 0.6 = -0.1
$$

**√âtape 2 : Ajout du biais**

$$
z = -0.1 + 0.1 = 0.0
$$

**√âtape 3 : Application de sigmoid**

$$
f(0.0) = \frac{1}{1 + e^{0}} = \frac{1}{1 + 1} = 0.5
$$

‚úÖ **R√©sultat attendu : 0.5** (ton code devrait afficher √ßa !)

***

## üéØ Ce que tu as r√©ussi √† faire

| **Avant (√âtape 1.3)** | **Maintenant (√âtape 1.4)** |
| :-- | :-- |
| Neurone calculait seulement $z$ | Neurone applique une **activation** |
| Toujours le m√™me type de sortie | **4 comportements diff√©rents** (identity, heaviside, sigmoid, relu) |
| Impossible de mod√©liser des probl√®mes complexes | **Pr√™t pour les r√©seaux profonds** ! |


***

## ‚úÖ Validation finale

Lance ton code :

```bash
python neuron.py
```

**R√©sultat attendu** :

```
==================================================
TESTS DES FONCTIONS D'ACTIVATION
==================================================
Sigmoid: 0.5
ReLU: 0.0
Heaviside (0,0): 0
Heaviside (1,1): 1
Identity: 0.0
==================================================
‚úÖ TOUS LES TESTS PASS√âS !
==================================================
```


***

## üöÄ Prochaine √©tape : JOUR 1 ‚Üí JOUR 2

Tu as **termin√© le JOUR 1** ! üéâ

**R√©cap de ce que tu ma√Ætrises** :

- ‚úÖ Classe `Neuron` fonctionnelle
- ‚úÖ Calcul du produit scalaire + biais
- ‚úÖ 4 fonctions d'activation impl√©ment√©es

**Prochaine mission (√âtape 2.1)** : Cr√©er une **couche de neurones** (Layer) pour passer de 1 neurone √† plusieurs neurones qui travaillent ensemble ![^1]

üí™ **Bravo, tu progresses comme un pro !** La base de ton r√©seau de neurones est solide. Continue comme √ßa ! üî•

<div align="center">‚ÅÇ</div>

[^1]: ROADMAP-TOYCEPTRON-Mode-Sprint-3-4-jours.md

