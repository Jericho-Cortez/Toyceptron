<img src="https://r2cdn.perplexity.ai/pplx-full-logo-primary-dark%402x.png" style="height:64px;margin-right:32px"/>

# Sprint Toyceptron - JOUR 3 dÃ©marrÃ©

Ã‰tape 3.2 : CoderÂ network.pyÂ - Version simple
Go ! ğŸš€

Excellent ! On attaque **l'Ã©tape 3.2 : network.py version simple**. C'est le moment de faire communiquer toutes tes couches entre elles ! ğŸ§ âš¡

## ğŸ¯ Concept : Le rÃ©seau = enchaÃ®nement de couches

Un rÃ©seau de neurones, c'est **simple** :

- **EntrÃ©e** â†’ passe dans **Layer 1** â†’ rÃ©sultat passe dans **Layer 2** â†’ ... â†’ **Sortie finale**
- Chaque couche transforme ses inputs et les transmet Ã  la suivante

Sur [TensorFlow Playground](https://playground.tensorflow.org), c'est **exactement Ã§a** : les colonnes de neurones reprÃ©sentent tes layers, et les valeurs "coulent" de gauche Ã  droite.[^1]

***

## ğŸ“ Architecture de `network.py`

Voici la structure attendue :

```python
from layer import Layer

class Network:
    def __init__(self, layer_sizes, activations):
        """
        layer_sizes : liste des tailles [nb_inputs, nb_hidden1, ..., nb_outputs]
                      Ex: [2, 3, 1] = 2 entrÃ©es, 3 neurones cachÃ©s, 1 sortie
        activations : liste des fonctions d'activation (une par couche)
                      Ex: [relu, sigmoid]
        """
        self.layers = []
        # TODO: crÃ©er les couches
    
    def forward(self, inputs):
        """
        Propage les inputs Ã  travers toutes les couches
        """
        # TODO: enchaÃ®ner les forward()
        pass
```


***

## âš™ï¸ Ã‰tapes de codage

### **1ï¸âƒ£ Initialisation : construire les couches**

Dans `__init__`, tu dois crÃ©er autant de `Layer` que nÃ©cessaire.

**Logique** :

- Si `layer_sizes = [2, 3, 1]` â†’ tu dois crÃ©er **2 layers** :
    - Layer 1 : **3 neurones** qui prennent **2 inputs** chacun
    - Layer 2 : **1 neurone** qui prend **3 inputs** (sorties du Layer 1)

**Code Ã  Ã©crire** :

```python
for i in range(len(layer_sizes) - 1):
    num_inputs = layer_sizes[i]
    num_neurons = layer_sizes[i + 1]
    activation = activations[i]
    
    layer = Layer(num_neurons, num_inputs, activation)
    self.layers.append(layer)
```

**Explication** :

- `len(layer_sizes) - 1` : si tu as 3 tailles, tu crÃ©es **2 layers**[^1]
- `layer_sizes[i]` = nombre d'entrÃ©es pour cette couche
- `layer_sizes[i+1]` = nombre de neurones dans cette couche
- Tu ajoutes chaque `Layer` Ã  `self.layers`[^1]

***

### **2ï¸âƒ£ Forward pass : faire circuler les donnÃ©es**

Dans `forward()`, tu dois **propager les inputs Ã  travers chaque couche**.

**Principe** :

- Commence avec `current = inputs`
- Pour chaque layer, fais `current = layer.forward(current)`
- Ã€ la fin, `current` contient la sortie finale

**Code Ã  Ã©crire** :

```python
def forward(self, inputs):
    current = inputs
    for layer in self.layers:
        current = layer.forward(current)
    return current
```

**Pourquoi Ã§a marche ?**

- `layer.forward()` retourne une **liste** (les sorties des neurones)
- Cette liste devient l'**input** de la couche suivante
- Dernier layer â†’ sortie finale du rÃ©seau[^1]

***

## ğŸ§ª Test immÃ©diat

Ajoute ce code **Ã  la fin de `network.py`** :

```python
if __name__ == "__main__":
    from activations import relu, sigmoid
    
    # RÃ©seau : 2 entrÃ©es â†’ 3 neurones cachÃ©s â†’ 1 sortie
    net = Network(layer_sizes=[2, 3, 1], activations=[relu, sigmoid])
    
    result = net.forward([1.0, 2.0])
    print(f"Sortie du rÃ©seau : {result}")
```

**RÃ©sultat attendu** :

- Une liste avec **1 valeur** (car 1 neurone en sortie)
- Ex : `[0.734821...]` (valeur bizarre car poids alÃ©atoires)[^1]

***

## âœ… Validation

Ton code fonctionne si :

1. âœ… Le test s'exÃ©cute sans erreur
2. âœ… Tu obtiens une liste de la bonne taille (ici `[valeur]`)
3. âœ… Tu peux changer `layer_sizes` et Ã§a s'adapte automatiquement

***

## ğŸ”¥ Fichier complet `network.py`

```python
from layer import Layer

class Network:
    def __init__(self, layer_sizes, activations):
        """
        Construit un rÃ©seau de neurones multi-couches.
        
        Args:
            layer_sizes: liste [nb_inputs, nb_hidden1, ..., nb_outputs]
            activations: liste de fonctions d'activation (une par couche)
        """
        self.layers = []
        
        # CrÃ©er chaque couche
        for i in range(len(layer_sizes) - 1):
            num_inputs = layer_sizes[i]
            num_neurons = layer_sizes[i + 1]
            activation = activations[i]
            
            layer = Layer(num_neurons, num_inputs, activation)
            self.layers.append(layer)
    
    def forward(self, inputs):
        """
        Propagation avant : fait passer les inputs Ã  travers toutes les couches.
        
        Args:
            inputs: liste de valeurs d'entrÃ©e
        
        Returns:
            Liste des sorties du rÃ©seau
        """
        current = inputs
        for layer in self.layers:
            current = layer.forward(current)
        return current


# Test rapide
if __name__ == "__main__":
    from activations import relu, sigmoid
    
    # RÃ©seau 2 â†’ 3 â†’ 1
    net = Network(layer_sizes=[2, 3, 1], activations=[relu, sigmoid])
    result = net.forward([1.0, 2.0])
    print(f"Sortie du rÃ©seau : {result}")
    
    # RÃ©seau plus complexe 4 â†’ 5 â†’ 3 â†’ 1
    net2 = Network(layer_sizes=[4, 5, 3, 1], activations=[relu, relu, sigmoid])
    result2 = net2.forward([1.0, 2.0, 3.0, 4.0])
    print(f"Sortie rÃ©seau complexe : {result2}")
```


***

## ğŸš€ Prochaine Ã©tape

Une fois que ton test fonctionne, passe Ã  **l'Ã©tape 3.3 : intÃ©gration avec `main.py`** pour vÃ©rifier que ton code est compatible avec le code fourni par ton prof ![^1]

ğŸ’ª **Tu viens de coder l'architecture complÃ¨te d'un rÃ©seau de neurones.** C'est Ã©norme. Tout est prÃªt pour la phase de validation !

<div align="center">â‚</div>

[^1]: ROADMAP-TOYCEPTRON-Mode-Sprint-3-4-jours.md

